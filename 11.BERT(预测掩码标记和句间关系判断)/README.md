# BERT

[toc]

## 程序步骤

1) 设置基本变量值,数据预处理
2) 构建输入样本
   1) 在样本集中随机选取a和b两个句子
   2) 把ab两个句子合并为1个模型输入句,在句首加入分类符CLS,在ab中间和句末加入分隔符SEP
   3) 在模型输入句中随机选取15%单词准备用于mask, 再在这个些选中的单词中,按照论文策略进行mask
   4) 把所有存储单词的变量都填充至最大长度(有利于统一处理)
   5) 判断句间关系(ab是否相邻)
3) **构建BERT模型**
   1) 按照论文图2构建输入encoder的嵌入矩阵, `Embedding: input = word + position + segment`
   2) 根据transformer的方法对输入句子进行padding mask, 在注意力机制中屏蔽掉填充字符
   3) 根据transformer模型,输入编码器,得到输出
   4) **Task1:** 根据记录的mask单词位置,从输出中提取相应位置模型预测的各单词概率,经过激活全连接归一化等处理,得到最终的语言模型(即一个嵌入矩阵)
   5) **Task2:** 从输出中提取第一列的元素值(即输入向量CLS对应的输出元素),经过激活全连接归一化等处理,得到最终分类输出
4) **训练:** 求任务1的损失值loss_lm和任务2的损失值loss_clsf的和,然后进行梯度下降,直到连续三次的loss都小于阈值0.01
5) **测试:** 遍历每个样本句子对,将其输入模型,从返回的logits_lm和logits_clsf中选择每行最大值,即为预测结果.
