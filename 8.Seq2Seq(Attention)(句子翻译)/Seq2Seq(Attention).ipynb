{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task: 基于Seq2Seq和注意力机制的句子翻译\n",
    "Author: ChengJunkai @github.com/Cheng0829\n",
    "Email: chengjunkai829@gmail.com\n",
    "Date: 2022/09/13\n",
    "Reference: Tae Hwan Jung(Jeff Jung) @graykode\n",
    "\"\"\"\n",
    "\n",
    "from tkinter import font\n",
    "import numpy as np\n",
    "import torch, time, os, sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# S: 表示开始进行解码输入的符号。\n",
    "# E: 表示结束进行解码输出的符号。\n",
    "# P: 当前批次数据大小小于时间步长时将填充空白序列的符号\n",
    "\n",
    "'''1.数据预处理'''\n",
    "def pre_process(sentences):\n",
    "    # 分词\n",
    "    word_sequence = \" \".join(sentences).split()\n",
    "    # 去重\n",
    "    word_list = []\n",
    "    '''\n",
    "    如果用list(set(word_sequence))来去重,得到的将是一个随机顺序的列表(因为set无序),\n",
    "    这样得到的字典不同,保存的上一次训练的模型很有可能在这一次不能用\n",
    "    (比如上一次的模型预测碰见我:0,,就输出i:7,但这次模型i在字典8号位置,也就无法输出正确结果)\n",
    "    '''\n",
    "    for word in word_sequence:\n",
    "        if word not in word_list:\n",
    "            word_list.append(word)\n",
    "    word_dict = {w:i for i, w in enumerate(word_list)}\n",
    "    number_dict = {i:w for i, w in enumerate(word_list)}\n",
    "    # 词库大小,也是嵌入向量维度\n",
    "    n_class = len(word_dict)  # 12\n",
    "    return word_list, word_dict, number_dict, n_class \n",
    "\n",
    "'''根据句子数据,构建词元的嵌入向量'''\n",
    "def make_batch(sentences,word_dict):\n",
    "    # [1, 6, 12] [样本数, 输入句子长度, 嵌入向量维度(单词类别数)]\n",
    "    input_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[0].split()]]]\n",
    "    # [1, 5, 12] [样本数, 输出句子长度, 嵌入向量维度(单词类别数)]\n",
    "    output_batch = [np.eye(n_class)[[word_dict[n] for n in sentences[1].split()]]]\n",
    "    # [1, 5] [样本数, 输出句子长度]\n",
    "    target_batch = [[word_dict[n] for n in sentences[2].split()]]\n",
    "\n",
    "    input_batch = torch.FloatTensor(np.array(input_batch)).to(device)\n",
    "    output_batch =torch.FloatTensor(np.array(output_batch)).to(device)\n",
    "    target_batch = torch.LongTensor(np.array(target_batch)).to(device)\n",
    "\n",
    "    return input_batch, output_batch, target_batch\n",
    "\n",
    "'''2.构建模型'''\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.decoder_cell = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        # Linear for attention\n",
    "        self.attn = nn.Linear(n_hidden, n_hidden)\n",
    "        self.out = nn.Linear(2*n_hidden, n_class)\n",
    "\n",
    "    '''output, _ = model(input_batch, hidden_0, output_batch)'''\n",
    "    def forward(self, encoder_inputs, hidden_0, decoder_inputs):\n",
    "        # [6, 1, 12] [输入句子长度(n_step), 样本数, 嵌入向量维度(单词类别数)]\n",
    "        encoder_inputs = encoder_inputs.transpose(0, 1)  # encoder_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
    "        # [5, 1, 12] [输出句子长度(n_step), 样本数, 嵌入向量维度(单词类别数)]\n",
    "        decoder_inputs = decoder_inputs.transpose(0, 1)  # decoder_inputs: [n_step(=n_step, time step), batch_size, n_class]\n",
    "        # print(encoder_inputs.shape, decoder_inputs.shape)\n",
    "        '''编码器encoder'''\n",
    "        # encoder_outputs : [实际的n_step, batch_size, num_directions(=1)*n_hidden] # [5,1,128]\n",
    "        # encoder_states : [num_layers*num_directions, batch_size, n_hidden] # [1,1,128]\n",
    "        '''encoder_states是最后一个时间步的输出(即隐藏层状态),和encoder_outputs的最后一个元素一样'''\n",
    "        encoder_outputs, encoder_states = self.encoder_cell(encoder_inputs, hidden_0)\n",
    "        encoder_outputs = encoder_outputs # [6,1,128]\n",
    "        encoder_states = encoder_states # [1,1,128]\n",
    "        # print(encoder_outputs.shape, encoder_states.shape)\n",
    "        n_step = len(decoder_inputs) # 5\n",
    "        # 返回一个未初始化的张量,内部均为随机数\n",
    "        output = torch.empty([n_step, 1, n_class]).to(device) # [5,1,12]\n",
    "        \n",
    "        '''获取注意力权重 : between(整个编码器上的隐状态, 整个解码器上的隐状态)\n",
    "        有两次加权求和,一次是bmm,一次是dot,对应两个for循环\n",
    "        '''\n",
    "        trained_attn = []\n",
    "        '''解码器上的每个时间步'''\n",
    "        for i in range(n_step): # 5\n",
    "            '''解码器'''\n",
    "            '''decoder_inputs[i]即只需要第i个时间步上面的解码器输入,但必须是三维,所以用unsqueeze升一维'''\n",
    "            decoder_input_one = decoder_inputs[i].unsqueeze(0) # 升维\n",
    "            '''decoder_output_one 和 encoder_states 其实是一样的 因为decoder_cell只算了一个时间步'''\n",
    "            decoder_output_one, encoder_states = self.decoder_cell(decoder_input_one, encoder_states)\n",
    "            decoder_output_one = decoder_output_one\n",
    "            encoder_states = encoder_states\n",
    "            '''attn_weights是一个解码器时间步隐状态和整个编码器之间的注意力权重'''\n",
    "            # attn_weights : [1, 1, n_step] # [1,1,6]\n",
    "            attn_weights = self.get_attn_one_to_all(decoder_output_one, encoder_outputs)\n",
    "            # \n",
    "            '''squeeze():[1,1,6]->[6,] data:只取数据部分,剔除梯度部分 numpy:转换成一维矩阵'''\n",
    "            trained_attn.append(attn_weights.squeeze().data.numpy())\n",
    "            # numpy遍历不能存在于cuda,所以必须先作为cpu变量进行操作,再进行转换\n",
    "            attn_weights = attn_weights.to(device) \n",
    "            \"\"\"a.bmm(b)和torch.bmm(a,b)一样\n",
    "                a:(z,x,y)\n",
    "                b:(z,y,c)\n",
    "                则result = torch.bmm(a,b),维度为:(z,x,c)\n",
    "            \"\"\"\n",
    "            '''利用attn第i时刻Encoder的隐状态的加权求和,得到上下文向量,即融合了注意力的模型输出'''\n",
    "            # context:[1,1,n_step(=5)]x[1,n_step(=5),n_hidden(=128)]=[1,1,128]\n",
    "            context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "            # decoder_output_one : [batch_size(=1), num_directions(=1)*n_hidden]\n",
    "            decoder_output_one = decoder_output_one.squeeze(0) # [1,1,128] -> [1,128]\n",
    "            # [1, num_directions(=1)*n_hidden] # [1,128]\n",
    "            context = context.squeeze(1)  \n",
    "            '''把上下文向量和解码器隐状态进行concat,得到融合了注意力的模型输出'''\n",
    "            # torch.cat的dim=1代表在第二个维度上拼接 ,所以[1,128]+[1,128]->[1,256]\n",
    "            # output[i] = self.out(torch.cat((decoder_output_one, context), 1))\n",
    "            output[i] = self.out(torch.cat((decoder_output_one, context), 1))\n",
    "        # output: [5,1,12] -> [1,5,12] -> [5,12]\n",
    "        return output.transpose(0, 1).squeeze(0), np.array(trained_attn)\n",
    "\n",
    "    '''获取注意力权重 : between(解码器的一个时间步的隐状态, 整个编码器上的隐状态)'''\n",
    "    def get_attn_one_to_all(self, decoder_output_one, encoder_outputs):  \n",
    "        n_step = len(encoder_outputs) # 6\n",
    "        attn_scores = torch.zeros(n_step)  # attn_scores : [n_step,] -> [6,]\n",
    "        \n",
    "        '''对解码器的每个时间步获取注意力权重'''\n",
    "        for i in range(n_step):\n",
    "            encoder_output_one = encoder_outputs[i]\n",
    "            attn_scores[i] = self.get_attn_one_to_one(decoder_output_one, encoder_output_one)\n",
    "\n",
    "        \"\"\"F.softmax(matrix,dim) 将scores标准化为0到1范围内的权重\n",
    "            softmax(x_i) = exp(x_i) / sum( exp(x_1) + ··· + exp(x_n) )\n",
    "            由于attn_scores是一维张量,所以F.softmax不用指定dim\n",
    "        \"\"\"\n",
    "        # .view(1,1,-1)把所有元素都压到最后一个维度上,把一维的张量变成三维的\n",
    "        return F.softmax(attn_scores).view(1, 1, -1) # [6,] -> [1,1,6]\n",
    "\n",
    "    '''获取注意力权重 : between(编码器的一个时间步的隐状态, 解码器的一个时间步的隐状态)'''\n",
    "    def get_attn_one_to_one(self, decoder_output_one, encoder_output_one):  \n",
    "        # decoder_output_one : [batch_size, num_directions(=1)*n_hidden] # [1,128]\n",
    "        # encoder_output_one : [batch_size, num_directions(=1)*n_hidden] # [1,128]\n",
    "        # score : [batch_size, n_hidden] -> [1,128]\n",
    "        score = self.attn(encoder_output_one)  \n",
    "        '''X.view(shape) \n",
    "        >>> X = torch.ones((3,2))\n",
    "        >>> X = X.view(2,3) # X形状变为(2,3)\n",
    "        >>> X = X.view(-1) # X形状变为一维\n",
    "        '''\n",
    "        # decoder_output_one : [n_step(=1), batch_size(=1), num_directions(=1)*n_hidden] -> [1,1,128]\n",
    "        # score : [batch_size, n_hidden] -> [1,128]\n",
    "        # 求点积\n",
    "        return torch.dot(decoder_output_one.view(-1), score.view(-1))  # inner product make scalar value\n",
    "\n",
    "def translate(sentences):\n",
    "    input_batch, output_batch, target_batch = make_batch(sentences,word_dict)\n",
    "    blank_batch = [np.eye(n_class)[[word_dict[n] for n in 'SPPPP']]]\n",
    "    # test_batch: [1,5,12] [batch_size,len_sen,dict_size]\n",
    "    test_batch = torch.FloatTensor(np.array(blank_batch)).to(device) \n",
    "    dec_inputs = torch.FloatTensor(np.array(blank_batch)).to(device) \n",
    "\n",
    "    '''贪婪搜索'''\n",
    "    for i in range(len(test_batch[0])):\n",
    "        # predict: [len_sen, dict_size] [5,12]\n",
    "        predict, trained_attn = model(input_batch, hidden_0, dec_inputs) \n",
    "        predict = predict.data.max(1, keepdim=True)[1] # [5,1] [sen_len,1]\n",
    "        # 覆盖之前的padding字符\n",
    "        dec_inputs[0][i][word_dict['P']] = 0\n",
    "        dec_inputs[0][i][predict[i][0]] = 1\n",
    "        \n",
    "    predict, trained_attn = model(input_batch, hidden_0, dec_inputs) \n",
    "    predict = predict.data.max(1, keepdim=True)[1] # [5,1] [sen_len,1]\n",
    "    decoded = [word_list[i] for i in predict]\n",
    "    real_decoded = decoded # 记录不清除特殊字符的decoded\n",
    "\n",
    "    '''清除特殊字符'''\n",
    "    '''训练集的target均以E结尾,所以模型输出最后一个值也会是E'''\n",
    "    if 'E' in decoded:\n",
    "        end = decoded.index('E') # 5\n",
    "        decoded = decoded[:end] # 删除结束符及之后的所有字符\n",
    "    else:\n",
    "        return # 报错\n",
    "    while(True):\n",
    "        if 'P' in decoded:\n",
    "            del decoded[decoded.index('P')] # 删除空白符\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # 把列表元素合成字符串\n",
    "    translated = ' '.join(decoded) \n",
    "    real_output = ' '.join(real_decoded) \n",
    "    return translated, real_output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # n_step = 5 # number of cells(= number of Step)\n",
    "    chars = 30 * '*'\n",
    "    n_hidden = 128 # number of hidden units in one cell\n",
    "    '''GPU比CPU慢的原因大致为:\n",
    "    数据传输会有很大的开销,而GPU处理数据传输要比CPU慢,\n",
    "    而GPU在矩阵计算上的优势在小规模神经网络中无法明显体现出来\n",
    "    '''\n",
    "    device = ['cuda:0' if torch.cuda.is_available() else 'cpu'][0]\n",
    "    sentences = ['我 想 喝 啤 酒 P', 'S i want a beer', 'i want a beer E']\n",
    "\n",
    "    '''1.数据预处理'''\n",
    "    word_list, word_dict, number_dict, n_class = pre_process(sentences)\n",
    "    input_batch, output_batch, target_batch = make_batch(sentences,word_dict)\n",
    "    # hidden_0 : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]\n",
    "    hidden_0 = torch.zeros(1, 1, n_hidden).to(device) # [1,1,128]\n",
    "\n",
    "    '''2.构建模型'''\n",
    "    model = Attention()\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    if os.path.exists('model_param.pt') == True:\n",
    "        # 加载模型参数到模型结构\n",
    "        model.load_state_dict(torch.load('model_param.pt', map_location=device))\n",
    "\n",
    "    '''3.训练'''\n",
    "    print('{}\\nTrain\\n{}'.format('*'*30, '*'*30))\n",
    "    loss_record = []\n",
    "    for epoch in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "        output, trained_attn = model(input_batch, hidden_0, output_batch)\n",
    "        output = output.to(device)\n",
    "        loss = criterion(output, target_batch.squeeze(0)) # .squeeze(0)降成1维\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss >= 0.0001: # 连续30轮loss小于0.01则提前结束训练\n",
    "            loss_record = []\n",
    "        else:\n",
    "            loss_record.append(loss.item())\n",
    "            if len(loss_record) == 30:\n",
    "                torch.save(model.state_dict(), 'model_param.pt')\n",
    "                break    \n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'Loss = {:.6f}'.format(loss))\n",
    "            torch.save(model.state_dict(), 'model_param.pt')\n",
    "\n",
    "    '''4.测试'''\n",
    "    print('{}\\nTest\\n{}'.format('*'*30, '*'*30))\n",
    "    input = sentences[0]\n",
    "    output, real_output = translate(input)\n",
    "    print(sentences[0].replace(' P', ''), '->', output)\n",
    "\n",
    "    '''5.可视化注意力权重矩阵'''\n",
    "    trained_attn = trained_attn.round(2)\n",
    "    fig = plt.figure(figsize=(len(input.split()), len(real_output.split()))) # (5,5)\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(trained_attn, cmap='viridis')\n",
    "    ax.set_xticklabels([''] + input.split(), \\\n",
    "        fontdict={'fontsize': 14}, fontproperties='SimSun') # 宋体\n",
    "    ax.set_yticklabels([''] + real_output.split(), \\\n",
    "        fontdict={'fontsize': 14}, fontproperties='SimSun')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8384712f827c73b71ec1d871aaffaaf3604c18a78a335b5adcc2028fc4cb4b02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
