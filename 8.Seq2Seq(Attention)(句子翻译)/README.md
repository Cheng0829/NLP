# Seq2Seq(Attention)

[toc]

## 1.理论

### 1.1 机器翻译

#### 1.1.1 模型输出结果处理

在解码过程之后,由输出结果获得预测单词y的方法有以下几种：

1) **贪婪法(Greedy Search):** 根据字面意思,就是经过softmax之后取argmax,这种方式是最简单的，但也存在着问题：**局部最优不一定是全局最优**
   > 入门项目实验所选择的均为贪婪法
2) **暴力搜索:** 这只是一种理论上的方法，但它的复杂度是$O(∣V∣^m)$,V是词库大小,显然是不可行的
3) **束搜索(Beam Search):** 结合了贪婪搜索和暴力搜索的特点,根据集束宽每次取概率最大的前k个,这样选取的最后结果是一棵树，我们只需要选取路径得分最大的即可。这种算法的复杂度虽然是指数级别的,但只要k不取太大,最后的复杂度可以控制在$O(k^n)$。
   1) **改进集束搜索:**
      1) **长度归一化:** 由于束搜索每次取k个值之后计算的概率会累积,所以长句的概率往往十分接近0造成数值下溢.因此我们实际中常常把概率记录成log(P),最大化log的求和公式
      2) **目标函数归一化:** 原来的目标函数中长句的概率很低,因此比如偏向于简短结果,因此可以通过除以翻译结果的单词书了,进行归一化,减小对输出长句结果的惩罚

#### 1.1.2 BLEU得分

BLEU得分常用来衡量机器翻译结果的好坏

**N-Gram:**
假设所使用的词组颗粒度为n,则$BLEU=\frac{\sum各个n元词组(去重)在参考句子中出现的最多次数}{n元词组总数(不去重)}$

**Example:**

**Reference 1:** The cat is on the mat.

**Reference 2:** There is a cat on the mat.

**Machine Translate output:** The cat cat the cat on the mat.

1) 设n=1,翻译结果共有4个词组:"the"、"cat"、"on"、"mat".
   1) 其中,"the"在两个参考句分别出现了2次和1次,取2;
   2) "cat"、"on"、"mat"均最多出现1次,取1;
   3) 因此,结果为(2+1+1+1)/8=5/8
2) 设n=2,翻译结果共有"the cat"、"cat the"、"cat on"、"on the"和"the mat"共5个词组.
   1) 其中,"the cat"、"cat on"、"on the"和"the mat"在两个句子在最多出现1次,取1;
   2) "cat the"没有出现,取0;
   3) 因此,结果为(1+1+1+1)/6=2/3 (注意,虽然在翻译结果中"the cat"有两个,算出现次数的时候只算一个,但是计算二元词组总数时,需要计算两次)

### 1.2 注意力模型

#### 1.2.1 Attention模型

注意力机制(Attention Mechanism)的本质是对于给定目标,通过生成一个权重系数对输入进行加权求和，来识别输入中哪些特征对于目标是重要的,哪些特征是不重要的;
为了实现注意力机制,我们将输入的原始数据看作<Key,Value>键值对的形式,根据给定的任务目标中的查询值Query计算Key与Query之间的相似系数,可以得到Value值对应的权重系数,即注意力权重,之后再用权重系数对Value值进行加权求和,即可得到输出.我们使用Q,K,V分别表示Query,Key和Value.
> 注意力机制在深度学习各个领域都有很多的应用.不过需要注意的是,注意力并不是一个统一的模型,它只是一个机制,在不同的应用领域有不同的实现方法。

- 注意力权重系数W的公式如下:$W=softmax⁡(QK^T)$
- 注意力权重系数W与Value做点积操作(加权求和)得到融合了注意力的输出:
$Attention(Q,K,V)=W⋅V=softmax⁡(QK^T)⋅V$

注意力模型的详细结构如下图所示:
![ ](img/Seq2Seq(Attention)-Model-Essence.png)
> 在本实验中Query是指decoder_output, Key和Value都是指encoder_outputs, 注意力权重W是指attn_weights

1) 在对decoder时间步的遍历循环中,用dot点积得到每个时间步decoder_output_one和encoder_outputs之间的注意力权重,最后生成注意力权重列表trained_attn
2) 把每一个decoder时间步生成的注意力权重attn_weights和encoder_outpus用bmm()函数进行加权求和,得到融合了注意力的输出context
3) 当然,在Seq2Seq任务中,求出的context还需要和decoder_output进行combine和fc

---

传统机器翻译和加了注意力机制之后的机器翻译的BLEU得分对比:
![ ](img/Seq2Seq(Attention)-BLEU.png)

#### 1.2.2 Seq2Seq(Attention)模型结构

在添加了注意力模型前后,编码器结构并没有什么变化;但是,在解码器中,传统的解码器结构发生了很大改变.

##### 1.2.2.1 Encoder

![ ](img/Seq2Seq(Attention)-Encoder.png)

---

##### 1.2.2.2 Decoder

###### 1.2.2.2.1 原始解码器

![ ](img/Seq2Seq(Attention)-Decoder-Classic.png)

###### 1.2.2.2.2 带有注意力机制的解码器

![ ](img/Seq2Seq(Attention)-Decoder-Attention.png)

### 1.3 特殊字符

在序列模型的处理中,我们往往会在解码器的输入添加开始符$S$,在输出添加结束符$E$,同时用空白符$P$把所有词/句填充至相同长度

- **空白符:** 填充至等长,便于统一操作;
- **开始符:** 添加开始符是因为解码器Decoder的第一个时间步没有来自上一个解码器时间步的输入(虽然Decoder的第一个时间步有编码器Encoder的输出作为输入,但并不是来自解码器Decoder的),为了各个时间步处理的统一性,选择了一个可学习的特殊字符进行填充,这样的效果比单纯的空白字符更好;
- **结束符:** 添加结束符是为了在预测单词时告诉模型终止输出.在训练集数据很多时,句子显然不可能都是等长的,翻译结果也应该不等长,为了控制翻译结果的长度,我们会在训练数据的target末尾加入结束符,这样翻译短句时,模型看见了结束符也就不会继续翻译了.(当然也可以不设置终止符,而设置一个最大输出长度,超过长度自动结束翻译输出)
  
   > 开始符和结束符在训练时都被当做普通的一个单词或者字符进行训练,而他们的位置是固定的,开始符$S$只出现在解码器的输入,结束符$E$只出现在解码器的输出.当预测时,我们只在编码器Encoder中有输入,而解码器Decoder的输入就是'SPPP···'

## 2.实验

### 2.1 实验步骤

1) **数据预处理**,得到字典、样本数等基本数据
2) **构建Seq2Seq(Attention)模型**,分别设置编解码器的输入
3) **训练**
   1) **代入数据**,输入编码器,然后输入解码器
   2) 得到模型输出值,取其中最大值的索引,找到字典中对应的字母,即为模型预测的下一个字母.
   3) 把模型输出值和真实值相比,求得误差损失函数,运用Adam动量法梯度下降
4) **测试:** 贪婪搜索
5) 可视化注意力权重矩阵

### 2.2 算法模型

#### 2.2.1 Encoder

![ ](img/Seq2Seq(Attention)-Test-Encoder.png)

---

#### 2.2.2 Decoder

![ ](img/Seq2Seq(Attention)-Test-Decoder.png)
