{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task: 基于Bi-LSTM的长句单词预测\n",
    "Author: ChengJunkai @github.com/Cheng0829\n",
    "Email: chengjunkai829@gmail.com\n",
    "Date: 2022/09/09\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch, os, sys, time, re\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "'''1.数据预处理'''\n",
    "def pre_process(sentence):\n",
    "    # sentence = re.sub(\"[.,!?\\\\-]\", '', sentence.lower()).split(' ') \n",
    "    word_list = []\n",
    "    '''\n",
    "    如果用list(set(word_sequence))来去重,得到的将是一个随机顺序的列表(因为set无序),\n",
    "    这样得到的字典不同,保存的上一次训练的模型很有可能在这一次不能用\n",
    "    (比如上一次的模型预测碰见a:0,b:1,就输出c:2,但这次模型c在字典3号位置,也就无法输出正确结果)\n",
    "    '''\n",
    "    for word in sentence.split():\n",
    "        if word not in word_list:\n",
    "            word_list.append(word)\n",
    "\n",
    "    word_dict = {w:i for i, w in enumerate(word_list)}\n",
    "    number_dict = {i:w for i, w in enumerate(word_list)}\n",
    "    print(word_dict)\n",
    "    word_dict[\"''\"] = len(word_dict)\n",
    "    number_dict[len(number_dict)] = \"''\"\n",
    "    n_class = len(word_dict) # 词库大小:48\n",
    "    max_len = len(sentence.split()) # 句子长度:70\n",
    "    # print(max_len)\n",
    "    return sentence, word_dict, number_dict, n_class, max_len\n",
    "\n",
    "'''根据句子数据,构建词元的嵌入向量及目标词索引'''\n",
    "def make_batch(sentence):\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    input_print = []\n",
    "    words = sentence.split()\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        input = [word_dict[n] for n in words[:(i+1)]]\n",
    "        input = input + [0] * (max_len - 1 - len(input))\n",
    "        # print(np.array(input).shape) # (69,)\n",
    "        target = word_dict[words[i+1]]\n",
    "        '''\n",
    "        input_batch:\n",
    "            由于要预测长句的每一个位置的单词,\n",
    "            所以除了最后一个单词只被预测之外,\n",
    "            所有单词都要参与预测.\n",
    "            因此,训练样本数为:句子长度70-1=69\n",
    "        target_batch:\n",
    "            一个列表,分别存储69个训练样本的目标单词\n",
    "        '''\n",
    "        input_print.append(input)\n",
    "        # np.eye(n_class)[input] : [69,48]\n",
    "        # print(np.eye(n_class)[input].shape)\n",
    "        input_batch.append(np.eye(n_class)[input]) \n",
    "        target_batch.append(target)\n",
    "    # print(np.array(input_print)\n",
    "    '''input_print: [69,69]'''\n",
    "    '''input_batch: [69,69,48]'''\n",
    "    input_batch = torch.FloatTensor(np.array(input_batch))\n",
    "    # print(input_batch.shape)\n",
    "    target_batch = torch.LongTensor(np.array(target_batch)) #(69,)\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)\n",
    "    return input_batch, target_batch\n",
    "\n",
    "'''2.构建模型'''\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        # n_class是词库大小,即嵌入向量维度:48\n",
    "        '''bidirectional=True'''\n",
    "        self.lstm = nn.LSTM(input_size=n_class, hidden_size=hidden_size, bidirectional=True)\n",
    "        self.W = nn.Linear(hidden_size*2, n_class, bias=False)\n",
    "        self.b = nn.Parameter(torch.ones(n_class))\n",
    "\n",
    "    def forward(self, X):\n",
    "        '''训练样本数:69, 时间步长度(每一样本长度):69'''\n",
    "        '''X:[batch_size, n_step, n_class] [样本数,时间步长度(每一样本长度),嵌入向量维度(词库大小)]'''\n",
    "        # input : [n_step, batch_size, n_class]\n",
    "        '''transpose转置 -> input:[69,69,48]'''\n",
    "        # input : [输入序列长度(时间步长度),样本数,嵌入向量维度]\n",
    "        input = X.transpose(0, 1) # [69,69,48]\n",
    "        # hidden_state : [num_layers*num_directions, batch_size, hidden_size]\n",
    "        # hidden_state : [层数*网络方向,样本数,隐藏层的维度(隐藏层神经元个数)]\n",
    "        hidden_state = torch.zeros(1*2, len(X), hidden_size).to(device)\n",
    "        # cell_state : [num_layers*num_directions, batch_size, hidden_size]\n",
    "        # cell_state : [层数*网络方向,样本数,隐藏层的维度(隐藏层神经元个数)]\n",
    "        cell_state = torch.zeros(1*2, len(X), hidden_size).to(device)\n",
    "        '''\n",
    "        一个Bi-LSTM细胞单元有三个输入,分别是$输入向量x^{<t>}、隐藏层向量a^{<t-1>}\n",
    "        和记忆细胞c^{<t-1>}$;\n",
    "        '''\n",
    "        '''outputs:[时间步长度(每一样本长度),训练样本数,隐藏层向量维度*2] -> [69,69,256]'''\n",
    "        # outputs:[69,69,256] final_hidden_state:[2,69,128] final_cell_state:[2,69,128]\n",
    "        outputs, (final_hidden_state, final_cell_state) = self.lstm(input, (hidden_state, cell_state))\n",
    "        outputs = outputs.to(device)\n",
    "        '''\n",
    "        由于是双向,outputs中各个值是由每一步的两个output拼接而成的,所以维度=2*128=256\n",
    "        final_hidden_state只有final_output的一半参数,所以不能替换\n",
    "        '''\n",
    "        final_output = outputs[-1] # [batch_size, hidden_size*2] -> [69, 256]\n",
    "        Y_t = self.W(final_output) + self.b  # Y_t : [batch_size, n_class]\n",
    "        return Y_t\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hidden_size = 128 # 隐藏层神经元个数(向量维度)\n",
    "    device = ['cuda:0' if torch.cuda.is_available() else 'cpu'][0]\n",
    "    sentence = (\n",
    "        'China is one of the four ancient civilizations in the world. '\n",
    "        'Around 5800 years ago,  Yellow River, the middle and lower reaches of Yangtze River, ' \n",
    "        'and the West Liaohe River showed signs of origin of civilization; '\n",
    "        'around 5,300 years ago, various regions of China entered the stage of civilization; '\n",
    "        'around 3,800 years ago, Central Plains formed a more advanced stage. '\n",
    "        'Mature form of civilization, and radiate cultural influence to Quartet;'\n",
    "    )\n",
    "    # 长句训练太麻烦,所以改用字母\n",
    "    sentence = 'a b c d e f g h i j k l m n o p q r s t u v w x y z'\n",
    "    # sentence = 'a b c d e f g h i'\n",
    "    '''1.数据预处理'''\n",
    "    sentence, word_dict, number_dict, n_class, max_len = pre_process(sentence)\n",
    "    input_batch, target_batch = make_batch(sentence)\n",
    "\n",
    "    '''2.构建模型'''\n",
    "    '''模型加载'''\n",
    "    model = BiLSTM()\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    if os.path.exists('model_param.pt') == True:\n",
    "        # 加载模型参数到模型结构\n",
    "        model.load_state_dict(torch.load('model_param.pt', map_location=device))\n",
    "\n",
    "    '''3.训练'''\n",
    "    print('{}\\nTrain\\n{}'.format('*'*30, '*'*30))\n",
    "    loss_record = []\n",
    "    for epoch in range(10000):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_batch)\n",
    "        '''output:[25,27] target_batch:[25]'''\n",
    "        loss = criterion(output, target_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if loss >= 0.01: # 连续30轮loss小于0.01则提前结束训练\n",
    "            loss_record = []\n",
    "        else:\n",
    "            loss_record.append(loss.item())\n",
    "            if len(loss_record) == 30:\n",
    "                torch.save(model.state_dict(), 'model_param.pt')\n",
    "                break     \n",
    "\n",
    "        if ((epoch+1) % 1000 == 0):\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'Loss = {:.6f}'.format(loss))\n",
    "            torch.save(model.state_dict(), 'model_param.pt')\n",
    "\n",
    "    '''4.测试'''\n",
    "    '''\n",
    "    本实验与之前实验的不同之处在于,把句子单词挨个进行分解,所以看似只有一个样本,\n",
    "    实际有max_len-1个样本,也就是说训练时预测了从首单词到尾单词前的所有单词,\n",
    "    所以输入\"a\"到输入\"a~y\"均可输出\"a~z\"\n",
    "\n",
    "    但由于样本少且高度相似,所以必须按照训练样本的位置进行预测,\n",
    "    因为权重训练的是如何由\"a\"推出\"b\",如何由\"a b\"推出\"a b c\"......\n",
    "    如果开始单词改成\"b\",则预测结果不会是\"c\"\n",
    "    '''\n",
    "    print('{}\\nTest\\n{}'.format('*'*30, '*'*30))\n",
    "    sentence = 'a b c'\n",
    "    print(sentence)\n",
    "    length = 10\n",
    "    while len(sentence.split()) < length:\n",
    "        words = sentence.split()\n",
    "        input_batch = []\n",
    "        input = []\n",
    "        # 把单词换成序号\n",
    "        for word in words:\n",
    "            if word not in word_dict:\n",
    "                word = \"''\" # 把不存在赋值为空字符串\n",
    "            input.append(word_dict[word])\n",
    "        # 填充\n",
    "        input = input + [0] * (max_len - 1 - len(input))\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        input_batch = torch.FloatTensor(np.array(input_batch))\n",
    "        input_batch = input_batch.to(device)\n",
    "        predict = model(input_batch).data.max(1, keepdim=True)[1]\n",
    "        sentence = sentence + ' ' + number_dict[predict.item()]\n",
    "        print(sentence)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8384712f827c73b71ec1d871aaffaaf3604c18a78a335b5adcc2028fc4cb4b02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
