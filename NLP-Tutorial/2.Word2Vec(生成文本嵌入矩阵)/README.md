# 词嵌入

[toc]

## 1.理论

### 1.1 为什么使用词嵌入?

- one-hot向量(长度为词库大小,去重排序,一个one-hot仅在单词序号处取1,其余均为0)可以表示词,但是各个单词的one-hot乘积均为0,也就是看不出关联.
- 所以可以用特征化的嵌入向量来表示单词(矩阵列不是序号,而是n个特征,所需空间远少于列长为词库大小的one-hot),然后根据t-SNE算法把n维数据嵌入到而且平面,可以直观看到一些词的相近程度

$嵌入向量=嵌入矩阵*onehot向量$

### 1.2 词嵌入的类比推理

- 观察向量man、woman、king和queen的嵌入向量，可以发现 $man-woman≈king-queen$, 则可以知道单词之间的类比；同时根据向量差也可以看出，man和woman、king和queen在哪些特征上有差别（例如gender）

- 两个单词之间的相似度可以用余弦相似度衡量

### 1.3 学习词嵌入

学习词嵌入 -> 建立一个语言模型:

**预测下一个单词** 初始化一个参数矩阵,乘以各单词的one-hot向量,得到嵌入向量,把这些嵌入向量作为输入x放进神经网络,假设嵌入向量长度为n,句子已知单词数为m,则softmax分类器得到的向量维度是m×n,相当于把几个嵌入向量叠加在一起(实际中会设置一个固定的历史窗口,例如每次只看前/后k个词(也可同时看前和后),即窗口大小=k,,softmax层输入向量维度=k×n)
需要在m*n个可能的输出中预测这个单词(每个隐藏层包括softmax层都有自己的参数W和b)

### 1.4 Word2Vec & Skip-Gram(跳字模型)

**采用Skip-Gram模型的Word2Vec算法:** 基本步骤与`1.3 学习词嵌入`相同。不同之处在于,每次给定一个上下文词,然后选择目标词,但是每次并不只选前k个词,而是在一定词距内随机选择一个词
(从目标字词推测原始语句)

### 1.5 分级&负采样

由于softmax层向量维度过大，因此实践中往往会采用分级或负采样两种方法降低计算成本

#### 1.5.1 分级

每次并不会直接精确到属于n个词中的哪一个词，而是采用二分类法，不断划分直至最后。分级方法的计算成本与log2(词汇表大小)成正比。
Tips:为了进一步降低成本，常用词往往会放到离树顶更近的位置。

#### 1.5.2 负采样

对于每一个上下文词，在已选择正确的目标词，即有正类样本的基础上，会特意生成一系列负类样本。相当于一个样本一次迭代进行了多次训练。

### 1.6 Glove词向量

Glove算法是一种十分简便的词嵌入算法.

- 设词库大小为n,设X_ij是单词i在单词j上下文(假设是左右各k个词)中出现的次数,即X_ij是一个能够获取单词i和j出现位置相近时或是彼此接近的频率的计数器
- 遍历训练集,计算单词i在不同单词j上下文中一共出现的总次数,利用梯度下降法将差距最小化
- $minimize{\sum_{i=1}^n}{\sum_{j=1}^n}f(X_{ij})(θ_i^Te_j+b_i+b_j'-logX_{ij})^2$

## 2.实验

### 2.1 实验步骤

1) **数据预处理**,得到单词字典、样本数等基本数据
2) **构建模型**
   1) **构建跳字模型**:遍历数据,依次把第2个词~倒数第2个词作为目标词,然后对于每个target依次选择前、后一个词作为上下文词,将每一对(目标词,上下文词)加入跳字模型
   2) **Word2Vec模型**:2个全连接层
3) **训练**
   1) 根据batch_size**随机抽取样本**,代入数据
   2) 把模型输出值和真实值相比,求得误差损失函数,运用Adam动量法梯度下降
4) **嵌入矩阵可视化**
