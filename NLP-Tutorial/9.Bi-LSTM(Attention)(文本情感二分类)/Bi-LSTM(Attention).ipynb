{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task: 基于Bi-LSTM和注意力机制的文本情感分类\n",
    "Author: ChengJunkai @github.com/Cheng0829\n",
    "Email: chengjunkai829@gmail.com\n",
    "Date: 2022/09/14\n",
    "Reference: Tae Hwan Jung(Jeff Jung) @graykode\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch, time, os, sys\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''1.数据预处理'''\n",
    "def pre_process(sentences):\n",
    "    word_sequence = \" \".join(sentences).split()\n",
    "    word_list = []\n",
    "    '''\n",
    "    如果用list(set(word_sequence))来去重,得到的将是一个随机顺序的列表(因为set无序),\n",
    "    这样得到的字典不同,保存的上一次训练的模型很有可能在这一次不能用\n",
    "    (比如上一次的模型预测碰见i:0,love:1,就输出you:2,但这次模型you在字典3号位置,也就无法输出正确结果)\n",
    "    '''\n",
    "    for word in word_sequence:\n",
    "        if word not in word_list:\n",
    "            word_list.append(word) \n",
    "    word_dict = {w:i for i, w in enumerate(word_list)}\n",
    "    word_dict[\"''\"] = len(word_dict)\n",
    "    word_list = word_list.append(\"''\")\n",
    "    vocab_size = len(word_dict) # 词库大小16\n",
    "    max_size = 0\n",
    "    for sen in sentences:\n",
    "        if len(sen.split()) > max_size:\n",
    "            max_size = len(sen.split()) # 最大长度3\n",
    "    for i in range(len(sentences)):\n",
    "        if len(sentences[i].split()) < max_size:\n",
    "            sentences[i] = sentences[i] + \" ''\" * (max_size - len(sentences[i].split()))\n",
    "    \n",
    "    return sentences, word_list, word_dict, vocab_size, max_size\n",
    "\n",
    "def make_batch(sentences):\n",
    "    # 对于每个句子,返回包含句子内每个单词序号的列表\n",
    "    inputs = [np.array([word_dict[n] for n in sen.split()]) for sen in sentences] # [6,3]\n",
    "    targets = [out for out in labels]\n",
    "    inputs = torch.LongTensor(np.array(inputs)).to(device)\n",
    "    targets = torch.LongTensor(np.array(targets)).to(device)\n",
    "    '''情感分类构建嵌入矩阵,没有eye()'''\n",
    "    return inputs, targets\n",
    "\n",
    "class BiLSTM_Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BiLSTM_Attention, self).__init__()\n",
    "        '''情感分类构建嵌入矩阵,没有eye()'''\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, n_hidden, bidirectional=True)\n",
    "        self.out = nn.Linear(2*n_hidden, num_classes)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # input : [batch_size, n_step, embedding_dim] [6,3,2]\n",
    "        input = self.embedding(X) \n",
    "        # input : [n_step, batch_size, embedding_dim] [3,6,2]\n",
    "        # input : [输入序列长度(时间步长度),样本数,嵌入向量维度]\n",
    "        input = input.permute(1, 0, 2) \n",
    "        # hidden_state : [num_layers(=1)*num_directions(=2), batch_size, n_hidden]\n",
    "        # hidden_state : [层数*网络方向,样本数,隐藏层的维度(隐藏层神经元个数)]\n",
    "        hidden_state = torch.zeros(1*2, len(X), n_hidden).to(device) \n",
    "        # cell_state : [num_layers*num_directions, batch_size, hidden_size]\n",
    "        # cell_state : [层数*网络方向,样本数,隐藏层的维度(隐藏层神经元个数)]\n",
    "        cell_state = torch.zeros(1*2, len(X), n_hidden).to(device) \n",
    "        # final_hidden_state, final_cell_state : [num_layers(=1)*num_directions(=2), batch_size, n_hidden]\n",
    "        ltsm_output, (final_hidden_state, final_cell_state) = self.lstm(input, (hidden_state, cell_state))\n",
    "        # ltsm_output : [batch_size, n_step, n_hidden*num_directions(=2)]\n",
    "        ltsm_output = ltsm_output.permute(1, 0, 2) \n",
    "        attn_output, attention = self.attention_net(ltsm_output, final_hidden_state)\n",
    "        # model : [batch_size, num_classes], attention : [batch_size, n_step]\n",
    "        return self.out(attn_output), attention \n",
    "\n",
    "    '''两次bmm加权求和,相当于两次for循环''' \n",
    "    # lstm_output : [batch_size, n_step, n_hidden*num_directions(=2)] [6,3,16]\n",
    "    # final_hidden_state : [num_layers(=1)*num_directions(=2), batch_size, n_hidden] [2,6,8]\n",
    "    def attention_net(self, lstm_output, final_hidden_state):\n",
    "        # final_hidden_state : [batch_size, n_hidden*num_directions(=2), 1(=n_layer)] [6,16,1]\n",
    "        final_hidden_state = final_hidden_state.view(-1, 2*n_hidden, 1) \n",
    "\n",
    "        '''第一次bmm加权求和:: lstm_output和final_hidden_state生成注意力权重attn_weights'''\n",
    "        # [6,3,16]*[6,16,1] -> [6,3,1] -> attn_weights : [batch_size, n_step] [6,3]\n",
    "        attn_weights = torch.bmm(lstm_output, final_hidden_state).squeeze(2) # 第3维度降维\n",
    "        softmax_attn_weights = F.softmax(attn_weights, 1) # 按列求值 [6,3]\n",
    "\n",
    "        '''第二次bmm加权求和 : lstm_output和注意力权重attn_weights生成上下文向量context,即融合了注意力的模型输出'''\n",
    "        # [batch_size, n_hidden*num_directions, n_step] * [batch_size,n_step,1] \\\n",
    "        # = [batch_size, n_hidden*num_directions, 1] : [6,16,3] * [6,3,1] -> [6,16,1] -> [6,16]\n",
    "        context = torch.bmm(lstm_output.transpose(1, 2), softmax_attn_weights.unsqueeze(2)).squeeze(2)\n",
    "        softmax_attn_weights = softmax_attn_weights.to('cpu') # numpy变量只能在cpu上\n",
    "        \n",
    "        '''各个任务求出context之后的步骤不同,LSTM的上下文不需要和Seq2Seq中的一样和decoder_output连接'''\n",
    "        return context, softmax_attn_weights.data.numpy() \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    chars = 30 * '*'\n",
    "    embedding_dim = 3 # embedding size\n",
    "    n_hidden = 8  # number of hidden units in one cell\n",
    "    num_classes = 2  # 0 or 1\n",
    "    '''GPU比CPU慢的原因大致为:\n",
    "    数据传输会有很大的开销,而GPU处理数据传输要比CPU慢,\n",
    "    而GPU在矩阵计算上的优势在小规模神经网络中无法明显体现出来\n",
    "    '''\n",
    "    device = ['cuda:0' if torch.cuda.is_available() else 'cpu'][0]\n",
    "    # 3 words sentences (=sequence_length is 3)\n",
    "    sentences = [\"i love you\", \"he loves me\", \"don't leave\", \\\n",
    "                 \"i hate you\", \"sorry for that\", \"this is awful\"]\n",
    "    labels = [1, 1, 1, 0, 0, 0]  # 1 is good, 0 is not good.\n",
    "\n",
    "    '''1.数据预处理'''\n",
    "    sentences, word_list, word_dict, vocab_size, max_size = pre_process(sentences)\n",
    "    inputs, targets = make_batch(sentences)\n",
    "    \n",
    "    '''2.构建模型'''\n",
    "    model = BiLSTM_Attention()\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    if os.path.exists('model_param.pt') == True:\n",
    "        # 加载模型参数到模型结构\n",
    "        model.load_state_dict(torch.load('model_param.pt', map_location=device))\n",
    "\n",
    "    '''3.训练'''\n",
    "    print('{}\\nTrain\\n{}'.format('*'*30, '*'*30))\n",
    "    loss_record = []\n",
    "    for epoch in range(10000):\n",
    "        optimizer.zero_grad()\n",
    "        output, attention = model(inputs)\n",
    "        output = output.to(device)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss)\n",
    "        if loss >= 0.001: # 连续30轮loss小于0.01则提前结束训练\n",
    "            loss_record = []\n",
    "        else:\n",
    "            loss_record.append(loss.item())\n",
    "            if len(loss_record) == 30:\n",
    "                torch.save(model.state_dict(), 'model_param.pt')\n",
    "                break    \n",
    "\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'Loss = {:.6f}'.format(loss))\n",
    "            torch.save(model.state_dict(), 'model_param.pt')\n",
    "\n",
    "    '''4.测试'''\n",
    "    print('{}\\nTest\\n{}'.format('*'*30, '*'*30))\n",
    "    test_text = 'sorry i hate you'\n",
    "    # 返回包含每个单词序号的列表矩阵(为了有2个维度,还要加一个中括号升维)\n",
    "    tests = [np.array([word_dict[n] for n in test_text.split()])]\n",
    "    test_batch = torch.LongTensor(np.array(tests)).to(device)\n",
    "    predict, attn_test = model(test_batch)\n",
    "    predict = predict.data.max(1, keepdim=True)[1]\n",
    "    print('The emotion of \"%s\" is '%test_text, end='')\n",
    "    if predict[0][0] == 0:\n",
    "        print('bad!')\n",
    "    else:\n",
    "        print('good!')\n",
    "\n",
    "    '''5.可视化注意力权重矩阵'''\n",
    "    fig = plt.figure(figsize=(0.5*len(sentences), 0.5*len(sentences[0]))) # [batch_size, n_step]\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    # attention : (6, 3)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    word_show = ['单词'] * len(sentences[0])\n",
    "    word_show = [word_show[i] + str(i+1) for i in range(len(sentences[0]))] # ['word_1', 'word_2', 'word_3']\n",
    "    ax.set_xticklabels([''] + word_show, fontdict={'fontsize': 14} , fontproperties='SimSun')\n",
    "    sentence_show = ['句子'] * len(sentences)\n",
    "    sentence_show = [sentence_show[i] + str(i+1) for i in range(len(sentence_show))] # ['sentence_1', 'sentence_2', 'sentence_3', 'sentence_4', 'sentence_5', 'sentence_6']\n",
    "    ax.set_yticklabels([''] + sentence_show, fontdict={'fontsize': 14}, fontproperties='SimSun')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8384712f827c73b71ec1d871aaffaaf3604c18a78a335b5adcc2028fc4cb4b02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
