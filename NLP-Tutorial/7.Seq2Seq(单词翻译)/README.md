# Seq2Seq

[toc]

## 1.理论

### 1.1 基本概念

在RNN模型需要解决的问题中，有一类`M to N`的问题，即输入输出不等长问题,例如机器翻译和生成概述。这种结构又叫做**Seq2Seq模型**，或者叫**Encoder-Decoder模型**。

### 1.2 模型结构

#### 1.2.1 Encoder

Encoder可以直接用一个RNN网络，它的主要任务是把输入数据编码并输出一个上下文向量$c$,可以直接用RNN的输出或最后一个隐状态向量$h_t$来得到$c$
![ ](img/Seq2Seq-Encoder.png)

#### 1.2.2 Decoder

Decoder也是一个RNN网络，它的主要任务是解码，把Encoder得到的上下文向量$c$作为其初始隐状态向量$h_0$,再根据输入$x$,得到输出结果
![ ](img/Seq2Seq-Decoder.png)

### 1.3 特殊字符

在序列模型的处理中,我们往往会在解码器的输入添加开始符$S$,在输出添加结束符$E$,同时用空白符$P$把所有词/句填充至相同长度

- **空白符:** 填充至等长,便于统一操作;
- **开始符:** 添加开始符是因为解码器Decoder的第一个时间步没有来自上一个解码器时间步的输入(虽然Decoder的第一个时间步有编码器Encoder的输出作为输入,但并不是来自解码器Decoder的),为了各个时间步处理的统一性,选择了一个可学习的特殊字符进行填充,这样的效果比单纯的空白字符更好;
- **结束符:** 添加结束符是为了在预测单词时告诉模型终止输出.在训练集数据很多时,句子显然不可能都是等长的,翻译结果也应该不等长,为了控制翻译结果的长度,我们会在训练数据的target末尾加入结束符,这样翻译短句时,模型看见了结束符也就不会继续翻译了.(当然也可以不设置终止符,而设置一个最大输出长度,超过长度自动结束翻译输出)
  
   > 开始符和结束符在训练时都被当做普通的一个单词或者字符进行训练,而他们的位置是固定的,开始符$S$只出现在解码器的输入,结束符$E$只出现在解码器的输出.当预测时,我们只在编码器Encoder中有输入,而解码器Decoder的输入就是'SPPP···'

## 2.实验

### 2.1 实验步骤

1) **数据预处理**,得到字典、样本数等基本数据
2) **构建Seq2Seq模型**,分别设置编解码器的输入
3) **训练**
   1) **代入数据**,输入编码器,然后输入解码器
   2) 得到模型输出值,取其中最大值的索引,找到字典中对应的字母,即为模型预测的下一个字母.
   3) 把模型输出值和真实值相比,求得误差损失函数,运用Adam动量法梯度下降
4) **测试**

### 2.2 算法模型

![ ](img/Seq2Seq-Train-Structure.png)
