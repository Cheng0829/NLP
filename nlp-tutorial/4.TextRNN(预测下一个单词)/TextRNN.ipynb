{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task: 基于TextRNN的单词预测\n",
    "Author: ChengJunkai @github.com/Cheng0829\n",
    "Date: 2022/09/08\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "'''1.数据预处理'''\n",
    "def pre_process(sentences):\n",
    "    # 分词\n",
    "    word_sequence = \" \".join(sentences).split()\n",
    "    # 去重\n",
    "    word_list = list(set(word_sequence))\n",
    "    # 字典\n",
    "    word_dict = {w:i for i, w in enumerate(word_list)}\n",
    "    word_dict[\"''\"] = len(word_dict)\n",
    "    number_dict = {i:w for i, w in enumerate(word_list)}\n",
    "    number_dict[len(number_dict)] = \"''\"\n",
    "    word_list.append(\"''\")\n",
    "    num_words = len(word_dict) # 词库大小:8\n",
    "    # 本实验不采用随机抽样,所以batch_size等于样本数\n",
    "    batch_size = len(sentences) # 样本数:5\n",
    "    # print(word_dict)\n",
    "    # print(number_dict)\n",
    "    # print(word_list)\n",
    "    return word_sequence, word_list, word_dict, number_dict, num_words, batch_size \n",
    "\n",
    "'''根据句子数据,构建词元的嵌入向量及目标词索引'''\n",
    "def make_batch(sentences,mode='train'):\n",
    "    # 和Word2Vec的random_batch基本一致,区别在于不随机\n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "\n",
    "    for sen in sentences:\n",
    "        words = sen.split() # 分词\n",
    "        for i in range(len(words)):\n",
    "            if(words[i] not in word_list):\n",
    "                words[i] = \"''\"\n",
    "\n",
    "        if mode == 'train':        \n",
    "            input = [word_dict[n] for n in words[:-1]] # 创建最后一个词之前所有词的序号列表\n",
    "            input_batch.append(np.eye(num_words)[input]) # 最后一个词之前所有词的嵌入向量\n",
    "            target = word_dict[words[-1]] # 每个目标词的序号\n",
    "            target_batch.append(target) # 记录每个目标词的序号\n",
    "        else:\n",
    "            input = [word_dict[n] for n in words] # 创建所有词的序号列表\n",
    "            input_batch.append(np.eye(num_words)[input]) # 所有词的嵌入向量\n",
    "    return torch.FloatTensor(np.array(input_batch)), torch.LongTensor(np.array(target_batch))\n",
    "\n",
    "'''2.构建模型:多对一RNN(本实验结构图详见笔记)'''\n",
    "class TextRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        '''nn.RNN(input_size, hidden_size, num_layers=1, bidirectional=False)\n",
    "        Args:\n",
    "            input_size : 嵌入向量维度\n",
    "            hidden_size : 隐藏层alpha维度(隐藏层的神经元个数)\n",
    "            num_layers=1 : 循环层数.设置num_aayers=2将意味着将两个RNN堆叠在一起,\n",
    "                其中第二个RNN接受第一个RNN的输出并计算最终结果(Default:1)\n",
    "            bidirectional: If ``True``, becomes a bidirectional RNN.(Default:False)\n",
    "        '''\n",
    "        '''nn.RNN:[嵌入向量维度, 隐藏层alpha维度]'''\n",
    "        # 即x的维度和α的维度\n",
    "        self.rnn = nn.RNN(input_size=num_words, hidden_size=hidden_size) # (8,4)\n",
    "        '''Weight:[隐藏层alpha维度, 嵌入向量维度]'''\n",
    "        self.Weight = nn.Linear(hidden_size, num_words, bias=False) # (4,8)\n",
    "        self.bias = nn.Parameter(torch.ones([num_words]))\n",
    "\n",
    "    '''每个样本输入的单词数和模型的时间步长度相等'''\n",
    "    def forward(self, X, hidden): # model(input_batch, hidden)\n",
    "        '''transpose(~) 矩阵转置\n",
    "        X(input_batch):[5,2,8] -> transpose -> [2,5,8]\n",
    "        '''\n",
    "        X = X.transpose(0,1) # 第0维和第1维转置\n",
    "        # X : [n_step, batch_size, num_words]\n",
    "        '''X:[输入序列长度(时间步长度),样本数,嵌入向量维度] -> [2,5,8]'''\n",
    "        '''hidden即为alpha'''\n",
    "        '''\n",
    "        RNN:(8,4) X:(2,5,8) hidden:(1,5,4) -> alpha_outputs:(2,5,4) alpha_t:(1,5,4)\n",
    "        alpha_t是最后一个时间步的输出 : [1,样本数,隐藏层alpha维度(隐藏层的神经元个数)] -> [1,5,4]\n",
    "        alpha_outputs存储所有时间步的输出,所以alpha_outputs[-1]和alpha_t值一样(除了前者[5,4]后者[1,5,4])\n",
    "        本实验为多对一,所以仅需alpha_t,若为多对多,则需要对alpha_outputs中每个alpha求y=W(alpha)+b\n",
    "        '''\n",
    "        alpha_outputs, alpha_t = self.rnn(X, hidden) # alpha_t:[batch_size, num_directions(=1)*hidden_size]\n",
    "        # Weight:[隐藏层alpha维度, 嵌入向量维度] alpha_t:[1,样本数,隐藏层alpha维度]\n",
    "        # Weight:[4,8] alpha_t:[1,5,4] Weight(alpha_t):[5,8]\n",
    "        # Y_t:[样本数,各单词的概率] -> [5,8] 最大值所在索引即为预测的单词索引\n",
    "        # alpha_t[0]==alpha_outputs[-1]\n",
    "        '''既可以使用alpha_t[0],也可以使用alpha_outputs[-1]'''\n",
    "        Y_t = alpha_outputs[-1] \n",
    "        Y_t = self.Weight(Y_t) + self.bias # self.bias:(num_words,) (8,)\n",
    "        return Y_t\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hidden_size = 4 # 隐藏层alpha维度(隐藏层的神经元个数)\n",
    "    sentences = [\"i like dog\", \"i love coffee\", \"i love coffee\", \"you love cloud\", \"i hate milk\"]\n",
    "    \n",
    "    '''1.数据预处理'''\n",
    "    word_sequence, word_list, word_dict, number_dict, num_words, batch_size = pre_process(sentences)\n",
    "    \n",
    "    '''2.构建模型'''\n",
    "    model = TextRNN()\n",
    "    criterion = nn.CrossEntropyLoss() # 交叉熵损失函数\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001) # Adam动量梯度下降法\n",
    "\n",
    "    '''根据句子数据,构建词元的嵌入向量及目标词索引'''\n",
    "    input_batch, target_batch = make_batch(sentences)\n",
    "\n",
    "    '''3.训练'''\n",
    "    for epoch in range(5000):\n",
    "        optimizer.zero_grad() # 把梯度置零，即把loss关于weight的导数变成0\n",
    "\n",
    "        # hidden : [num_layers(=1)*num_directions(=1), batch_size, hidden_size]\n",
    "        '''hidden:[层数*网络方向,样本数,隐藏层的维度(隐藏层神经元个数)] -> [1,5,4]'''\n",
    "        # α_0常以零向量输入\n",
    "        hidden = torch.zeros(1, batch_size, hidden_size) # (1,5,4)\n",
    "        \n",
    "        '''input_batch:[样本数,输入序列长度(时间步长度), 嵌入向量维度] -> [5,2,8]'''\n",
    "        output = model(input_batch, hidden) # [batch_size, num_words]\n",
    "        loss = criterion(output, target_batch) # 将输出与真实目标值对比,得到损失值\n",
    "        loss.backward() # 将损失loss向输入侧进行反向传播，梯度累计\n",
    "        optimizer.step() # 根据优化器对W、b和WT、bT等参数进行更新(例如Adam和SGD)\n",
    "        if ((epoch+1) % 1000 == 0):\n",
    "            print('Epoch:%d' % (epoch+1), 'cost=%.6f' % loss)\n",
    "\n",
    "    '''4.预测'''\n",
    "    sentences = [\"i like\", \"i hate\", \"you love\", \"you love my\", \"you\"]\n",
    "    for sen in sentences: # 每个样本逐次预测,避免长度不同\n",
    "        hidden = torch.zeros(1, 1, hidden_size)\n",
    "        input_batch, target_batch = make_batch([sen], mode='predict')\n",
    "        predict = model(input_batch, hidden).data.max(1, keepdim=True)[1]\n",
    "        result = predict.squeeze().item() # tensor([[~]]) -> tensor(~) -> ~\n",
    "        print(sen + ' -> ' + number_dict[result])\n",
    "\n",
    "'''\n",
    "为什么训练集句子长度都是2,但是测试集可以不是?\n",
    "    make_batch的input_batch维度是[batch_size(样本数), n_step(样本单词数),n_class]\n",
    "    n_step是输入序列长度,之前疑惑为什么只有2个rnn单元,却可以输入其他个数的字母,\n",
    "\n",
    "    实际上,模型并没有把时间步作为一个超参数,也就是时间步随输入样本而变化,在训练集中,n_step均为2,\n",
    "    但是,在测试集中,三个单词都是分别作为样本集输入的,也就是时间步分别为2,2,2,3,1\n",
    "    最后在self.rnn(X, hidden)中,模型会自动根据X的序列长度,分配时间步\n",
    "\n",
    "    但由于是一次性输入一个样本集,所以样本集中各个样本长度必须一致,否则报错\n",
    "    因此必须把预测的sentences中各个句子分别放进容量为1的样本集单独输入\n",
    "\n",
    "    需要指出的是,由于模型训练的是根据2个单词找到最后以1个单词,训练的是2个时间步之间的权重,\n",
    "    所以如果长度不匹配,即使单词在训练集中,也不能取得好的结果,比如\"you\"的预测结果不是训练集中的\"love\"\n",
    "'''\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8384712f827c73b71ec1d871aaffaaf3604c18a78a335b5adcc2028fc4cb4b02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
