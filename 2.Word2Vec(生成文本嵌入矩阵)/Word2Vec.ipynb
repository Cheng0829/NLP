{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task: 生成单词的特征嵌入矩阵,并进行图形化显示\n",
    "Author: ChengJunkai @github.com/Cheng0829\n",
    "Email: chengjunkai829@gmail.com\n",
    "Date: 2022/09/05\n",
    "Reference: Tae Hwan Jung(Jeff Jung) @graykode\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch, os, sys, time\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''1.数据预处理'''\n",
    "def pre_process(sentences):\n",
    "    # 分词\n",
    "    word_sequence = \" \".join(sentences).split() # ['apple', 'banana', 'fruit', 'banana', 'orange', 'fruit', 'orange', 'banana', 'fruit', 'dog', 'cat', 'animal', 'cat', 'monkey', 'animal', 'monkey', 'dog', 'animal']\n",
    "    # 去重\n",
    "    word_list = []\n",
    "    '''\n",
    "    如果用list(set(word_sequence))来去重,得到的将是一个随机顺序的列表(因为set无序),\n",
    "    这样得到的字典不同,保存的上一次训练的模型很有可能在这一次不能用\n",
    "    (比如上一次的模型预测碰见i:0,love:1,就输出you:2,但这次模型you在字典3号位置,也就无法输出正确结果)\n",
    "    '''\n",
    "    for word in word_sequence:\n",
    "        if word not in word_list:\n",
    "            word_list.append(word)\n",
    "    # 生成字典\n",
    "    word_dict = {w:i for i,w in enumerate(word_list)} # 注意:单词是键,序号是值 \n",
    "    # 词库大小:8\n",
    "    vocab_size = len(word_list) \n",
    "    return word_sequence, word_list, word_dict, vocab_size\n",
    "\n",
    "'''2-1:原版跳字模型'''\n",
    "def Skip_Grams_original(sentences, word_sequence, word_dict):\n",
    "    '''\n",
    "    原版:不同语句之间的边界词,会组成(target,context)\n",
    "    '''\n",
    "    '''\n",
    "    固定窗口大小(前后各一个词)\n",
    "    依次把第2个词~倒数第2个词作为目标词,然后对于每个target依次选择前、后一个词作为上下文词.\n",
    "    将每一对(目标词,上下文词)加入跳字模型\n",
    "    '''\n",
    "    skip_grams = []\n",
    "    for i in range(1, len(word_sequence)-1):\n",
    "        target = word_dict[word_sequence[i]] # 目标词序号键值\n",
    "        context_1 = word_dict[word_sequence[i-1]] # 目标词的前一个词的序号\n",
    "        context_2 = word_dict[word_sequence[i+1]] # 目标词的后一个词的序号\n",
    "        # 添加(目标词,上下文词)样本对\n",
    "        skip_grams.append([target, context_1]) \n",
    "        skip_grams.append([target, context_2])\n",
    "    return skip_grams\n",
    "\n",
    "'''2-1:跳字模型-cjk'''\n",
    "def Skip_Grams(sentences, word_sequence, word_dict):\n",
    "    '''\n",
    "    对于原版进行了一些修改,当构建跳字模型时，\n",
    "    仅在一个语句内挑选(target,context),不同句子之间不会产生输入输出对\n",
    "    '''\n",
    "    '''\n",
    "    固定窗口大小(前后各一个词)\n",
    "    依次把第2个词~倒数第2个词作为目标词,然后对于每个target依次选择前、后一个词作为上下文词.\n",
    "    将每一对(目标词,上下文词)加入跳字模型\n",
    "    '''\n",
    "    skip_grams = []\n",
    "    for i in range(len(sentences)):\n",
    "        sentences_i_list = sentences[i].split(' ')\n",
    "        length_sentences = len(sentences_i_list)\n",
    "        for j in range(1,length_sentences-1):\n",
    "            target = word_dict[sentences_i_list[j]] # 目标词序号键值\n",
    "            context_1 = word_dict[sentences_i_list[j-1]] # 目标词的前一个词的序号\n",
    "            context_2 = word_dict[sentences_i_list[j+1]] # 目标词的后一个词的序号\n",
    "            skip_grams.append([target, context_1]) \n",
    "            skip_grams.append([target, context_2])\n",
    "    return skip_grams\n",
    "\n",
    "'''2-2:Word2Vec模型'''\n",
    "class Word2Vec(nn.Module): # nn.Module是Word2Vec的父类\n",
    "    def __init__(self): # (输入/输出层大小，嵌入层大小)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        继承父类的所有方法(),比如nn.Module的add_module()和parameters()\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        '''2个全连接层\n",
    "        troch.nn.Linear(in_features_size, out_features_size)\n",
    "        输入向量:(batch_size, in_features_size)\n",
    "        输出向量:(batch_size, out_features_size)\n",
    "        '''\n",
    "        self.W = nn.Linear(vocab_size, embedding_size, bias=False) # 输入层:vocab_size > embedding_size Weight\n",
    "        self.WT = nn.Linear(embedding_size, vocab_size, bias=False) # 输出层:embedding_size > vocab_size Weight\n",
    "        # Tips:W和WT不是转换关系\n",
    "\n",
    "    def forward(self, X): # input_batch\n",
    "        hidden_layer = self.W(X)  \n",
    "        hidden_layer = hidden_layer.to(device)\n",
    "        output_layer = self.WT(hidden_layer)\n",
    "        output_layer = output_layer.to(device)\n",
    "        ''' X --W--> hidden_layer --WT--> output_layer '''\n",
    "        # X:[batch_size, vocab_size]\n",
    "        # hidden_layer:[batch_size, embedding_size]\n",
    "        # output_layer:[batch_size, vocab_size]        \n",
    "        return output_layer\n",
    "\n",
    "'''3-1:从跳字模型中随机抽样输入输出对(target,context),构建输入输出向量矩阵'''\n",
    "def random_batch(skip_grams, batch_size, vocab_size): \n",
    "    input_batch = []\n",
    "    target_batch = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False)\n",
    "    \"\"\"np.random.choice(L,num,replace=True):从数组/列表/元组中随机抽取num个元素\n",
    "    L:数组/列表/元组\n",
    "    num:选择元素的个数\n",
    "    replace=True(Default)则可以取相同数字; replace=False则不能取相同数字\n",
    "    \"\"\"\n",
    "    for i in random_index:\n",
    "        # target\n",
    "        # np.eye(8)生成8x8对角矩阵 -> one-hot向量\n",
    "        input_batch.append(np.eye(vocab_size)[skip_grams[i][0]])  \n",
    "        # context\n",
    "        target_batch.append(skip_grams[i][1])\n",
    "    \"\"\"\n",
    "    input_batch存储随机选择的target词向量,作为神经网络的输入\n",
    "    target_batch存储对应的context单词,作为真实的输出\n",
    "    由输入input_batch得到的输出output与目标值(真实输出)target_batch相比较,即可得到损失函数值\n",
    "    \"\"\"   \n",
    "    input_batch = np.array(input_batch)   # (batch_size, vocab_size)\n",
    "    target_batch = np.array(target_batch) # (batch_size)\n",
    "    # type: array -> tensor\n",
    "    '''input_batch包含batch_size个one-hot向量'''\n",
    "    input_batch = torch.Tensor(input_batch)        # [batch_size, vocab_size]\n",
    "    '''target_batch包含batch_size个context词的序号'''\n",
    "    target_batch = torch.LongTensor(target_batch)  # [batch_size]\n",
    "\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device)    \n",
    "    return input_batch, target_batch \n",
    "    \"\"\"i.e.\n",
    "    input_batch: \n",
    "        [[0. 1. 0. 0. 0. 0. 0. 0.],\n",
    "        [0. 0. 0. 0. 0. 0. 0. 1.]] \n",
    "    target_batch:    \n",
    "        [6 0] \n",
    "    即选出的输入输出对是(2,6)和(7,0)\n",
    "    \"\"\"\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = ['cuda:0' if torch.cuda.is_available() else 'cpu'][0]\n",
    "    embedding_size = 3  # 嵌入矩阵大小,即样本特征数,即嵌入向量的\"长度\"\n",
    "    batch_size = 2      # 批量大小\n",
    "    \"\"\"batch_size:表示单次传递给程序用以训练的参数个数\n",
    "    假设训练集有1000个数据,若设置batch_size=100,那么程序首先会用数据集第1-100个数据来训练模型。\n",
    "    当训练完成后更新权重,再使用第101-200的个数据训练,用完训练集中的1000个数据后停止\n",
    "    Pros:可以减少内存的使用、提高训练的速度(因为每次完成训练后都会更新权重值)\n",
    "    Cons:使用少量数据训练时可能因为数据量较少而造成训练中的梯度值较大的波动。\n",
    "    \"\"\"\n",
    "\n",
    "    chars = '*' * 20\n",
    "    sentences = [\"apple banana fruit\", \"banana orange fruit\", \"orange banana fruit\",\n",
    "                 \"dog cat animal\", \"cat monkey animal\", \"monkey dog animal\"]\n",
    "    '''1.数据预处理'''\n",
    "    word_sequence, word_list, word_dict, vocab_size = pre_process(sentences)\n",
    "\n",
    "    '''2.构建模型'''\n",
    "\n",
    "    '''2-1:构建跳字模型Skip_Grams''' # 上下一个词,[(target,context)......]\n",
    "    skip_grams = Skip_Grams(sentences, word_sequence, word_dict)\n",
    "\n",
    "    '''2-2:构建模型''' # Word2Vec():两个全连接层 input->hidden->output\n",
    "    model = Word2Vec() # (输入/输出层大小，嵌入层大小)\n",
    "    model.to(device)\n",
    "    # 交叉熵损失函数,用于解决二分类或多分类问题,其内部会自动加上Softmax层\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Adam动量法比随机梯度下降SGD更好\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    '''\n",
    "    >>> model.parameters()) \n",
    "    (W): Linear(in_features=8, out_features=2, bias=False)\n",
    "    (WT): Linear(in_features=2, out_features=8, bias=False)\n",
    "    '''\n",
    "\n",
    "    if os.path.exists('model_param.pt') == True:\n",
    "        # 加载模型参数到模型结构\n",
    "        model.load_state_dict(torch.load('model_param.pt', map_location=device))\n",
    "\n",
    "    '''3.训练'''\n",
    "    loss_record = []\n",
    "    for epoch in range(50000):\n",
    "        '''3-1:从跳字模型中随机抽样输入输出对(target,context),构建输入输出向量矩阵'''\n",
    "        input_batch, target_batch = random_batch(skip_grams, batch_size, vocab_size)\n",
    "        # print(input_batch, target_batch)\n",
    "        \n",
    "        '''3-2:导入模型进行(代码格式固定)'''\n",
    "        # model->forward()\n",
    "        output = model(input_batch) \n",
    "        output = output.to(device)\n",
    "        optimizer.zero_grad() # 把梯度置零，即把loss关于weight的导数变成0.\n",
    "        # output : [batch_size, vocab_size]\n",
    "        # target_batch : [batch_size,] (LongTensor, not one-hot)\n",
    "        loss = criterion(output, target_batch) # 将输出与真实目标值对比,得到损失值\n",
    "        loss.backward() # 将损失loss向输入侧进行反向传播，梯度累计\n",
    "        optimizer.step() # 根据优化器对W、b和WT、bT等参数进行更新(例如Adam动量法和随机梯度下降法SGD)\n",
    "        \n",
    "        if loss >= 0.01: # 连续30轮loss小于0.01则提前结束训练\n",
    "            loss_record = []\n",
    "        else:\n",
    "            loss_record.append(loss.item())\n",
    "            if len(loss_record) == 30:\n",
    "                torch.save(model.state_dict(), 'model_param.pt')\n",
    "                break    \n",
    "\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'Loss = {:.6f}'.format(loss))\n",
    "            torch.save(model.state_dict(), 'model_param.pt')  \n",
    "    \n",
    "    W, WT = model.parameters() # W即为嵌入矩阵\n",
    "\n",
    "    '''4.Draw'''\n",
    "    print(input_batch)\n",
    "    print(W, WT)\n",
    "    # 根据嵌入矩阵,可视化各个单词的特征值\n",
    "    for i, word in enumerate(word_list): # 枚举词库中每个单词\n",
    "        x, y = W[0][i].item(), W[1][i].item()\n",
    "        plt.scatter(x, y) # 散点图\n",
    "        plt.annotate(word, xy=(x,y))\n",
    "        '''plt.annotate(s='str',xy=(x,y)......)函数用于标注文字\n",
    "        s:文本注释内容\n",
    "        xy:被注释的坐标点\n",
    "        color 设置字体颜色 color={'b', 'g', 'r', 'c', 'm', 'y', 'k', 'w'}\n",
    "        '''\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8384712f827c73b71ec1d871aaffaaf3604c18a78a335b5adcc2028fc4cb4b02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
