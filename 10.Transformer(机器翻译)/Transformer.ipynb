{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task: 基于Transformer的句子翻译\n",
    "Author: ChengJunkai @github.com/Cheng0829\n",
    "Email: chengjunkai829@gmail.com\n",
    "Date: 2022/09/17\n",
    "Reference: Tae Hwan Jung(Jeff Jung) @graykode\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch, time, itertools, os, sys \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# S: 表示开始进行解码输入的符号.\n",
    "# E: 表示结束进行解码输出的符号.\n",
    "# P: 当前批次数据大小小于时间步长时将填充空白序列的符号\n",
    "\n",
    "'''1.数据预处理'''\n",
    "def pre_process(sentences):\n",
    "    # P在第一个,方便处理\n",
    "    src_sequence = ['P']\n",
    "    src_sequence.extend(sentences[0].split())\n",
    "    src_list = []\n",
    "    '''\n",
    "    如果用list(set(word_sequence))来去重,得到的将是一个随机顺序的列表(因为set无序),\n",
    "    这样得到的字典不同,保存的上一次训练的模型很有可能在这一次不能用\n",
    "    (比如上一次的模型预测碰见i:0,love:1,就输出you:2,但这次模型you在字典3号位置,也就无法输出正确结果)\n",
    "    '''\n",
    "    for word in src_sequence:\n",
    "        if word not in src_list:\n",
    "            src_list.append(word)\n",
    "    src_dict = {w:i for i,w in enumerate(src_list)}\n",
    "    src_dict_size = len(src_dict)\n",
    "    src_len = len(sentences[0].split()) # length of source\n",
    "\n",
    "    # P在第一个,方便处理\n",
    "    tgt_sequence = ['P']\n",
    "    tgt_sequence.extend(sentences[1].split()+sentences[2].split())\n",
    "    tgt_list = []\n",
    "    '''\n",
    "    如果用list(set(word_sequence))来去重,得到的将是一个随机顺序的列表(因为set无序),\n",
    "    这样得到的字典不同,保存的上一次训练的模型很有可能在这一次不能用\n",
    "    (比如上一次的模型预测碰见i:0,love:1,就输出you:2,但这次模型you在字典3号位置,也就无法输出正确结果)\n",
    "    '''\n",
    "    for word in tgt_sequence:\n",
    "        if word not in tgt_list:\n",
    "            tgt_list.append(word)\n",
    "    tgt_dict = {w:i for i,w in enumerate(tgt_list)}\n",
    "    number_dict = {i:w for i,w in enumerate(tgt_dict)}\n",
    "    tgt_dict_size = len(tgt_dict)\n",
    "    tgt_len = len(sentences[1].split()) # length of target\n",
    "\n",
    "    return src_dict,src_dict_size,tgt_dict,number_dict,tgt_dict_size,src_len,tgt_len\n",
    "\n",
    "'''根据句子数据,构建词元的输入向量'''\n",
    "def make_batch(sentences):\n",
    "    input_batch = [[src_dict[n] for n in sentences[0].split()]]\n",
    "    output_batch = [[tgt_dict[n] for n in sentences[1].split()]]\n",
    "    target_batch = [[tgt_dict[n] for n in sentences[2].split()]]\n",
    "    input_batch = torch.LongTensor(np.array(input_batch)).to(device)\n",
    "    output_batch = torch.LongTensor(np.array(output_batch)).to(device)\n",
    "    target_batch = torch.LongTensor(np.array(target_batch)).to(device)\n",
    "    # print(input_batch, output_batch,target_batch) # tensor([[0, 1, 2, 3, 4, 5]]) tensor([[3, 1, 0, 2, 4]]) tensor([[1, 0, 2, 4, 5]])\n",
    "    return input_batch, output_batch,target_batch\n",
    "\n",
    "def get_position_encoding_table(n_position, d_model): \n",
    "    # inputs: (src_len+1, d_model) or (tgt_len+1, d_model)\n",
    "    pos_table = np.zeros((n_position, d_model))\n",
    "    for pos in range(n_position):\n",
    "        for i in range(d_model):\n",
    "            tmp = pos / np.power(10000, 2*(i//2) / d_model)\n",
    "            if i % 2 == 0: \n",
    "                # 偶数为正弦\n",
    "                pos_table[pos][i] = np.sin(tmp) # (7 or 6, 512)\n",
    "            else:\n",
    "                # 奇数为余弦\n",
    "                pos_table[pos][i] = np.cos(tmp) # (7 or 6, 512)\n",
    "\n",
    "    return torch.FloatTensor(pos_table).to(device)\n",
    "\n",
    "def get_attn_pad_mask(seq_q, seq_k, dict): \n",
    "    '''mask大小和(len_q,len_k)一致,\n",
    "    是为了在点积注意力中,与torch.matmul(Q,K)的大小一致'''\n",
    "    # (seq_q, seq_k): (dec_inputs, enc_inputs) \n",
    "    # dec_inputs:[batch_size, tgt_len] # [1,5]\n",
    "    # enc_inputs:[batch_size, src_len] # [1,6]\n",
    "    batch_size, len_q = seq_q.size() # 1,5\n",
    "    batch_size, len_k = seq_k.size() # 1,6\n",
    "    \"\"\"Tensor.data.eq(element)\n",
    "    eq即equal,对Tensor中所有元素进行判断,和element相等即为True,否则为False,返回二值矩阵\n",
    "    Examples:\n",
    "        >>> tensor = torch.FloatTensor([[1, 2, 3], [4, 5, 6]])\n",
    "        >>> tensor.data.eq(1) \n",
    "        tensor([[ True, False, False],\n",
    "                [False, False, False]])\n",
    "    \"\"\"\n",
    "    # eq(zero) is PAD token\n",
    "    pad_attn_mask = seq_k.data.eq(dict['P']).unsqueeze(1) # 升维 enc: [1,6] -> [1,1,6]\n",
    "    # 矩阵扩充: enc: pad_attn_mask: [1,1,6] -> [1,5,6]\n",
    "    return pad_attn_mask.expand(batch_size, len_q, len_k) # batch_size, len_q, len_k\n",
    "\n",
    "'''Attention = Softmax(Q * K^T) * V '''\n",
    "def Scaled_Dot_Product_Attention(Q, K, V, attn_mask): \n",
    "    # Q_s: [batch_size, n_heads, len_q, d_k] # [1,8,5,64]\n",
    "    # K_s: [batch_size, n_heads, len_k, d_k] # [1,8,6,64]\n",
    "    # attn_mask: [batch_size, n_heads, len_q, len_k] # [1,8,5,6]\n",
    "\n",
    "    \"\"\"torch.matmul(Q, K)\n",
    "        torch.matmul是tensor的乘法,输入可以是高维的.\n",
    "        当输入是都是二维时,就是普通的矩阵乘法.\n",
    "        当输入有多维时,把多出的一维作为batch提出来,其他部分做矩阵乘法.\n",
    "        Exeamples:\n",
    "            >>> a = torch.ones(3,4)\n",
    "            >>> b = torch.ones(4,2)\n",
    "            >>> torch.matmul(a,b).shape\n",
    "            torch.Size([3,2])   \n",
    "\n",
    "            >>> a = torch.ones(5,3,4)\n",
    "            >>> b = torch.ones(4,2)\n",
    "            >>> torch.matmul(a,b).shape\n",
    "            torch.Size([5,3,2])\n",
    "\n",
    "            >>> a = torch.ones(2,5,3)\n",
    "            >>> b = torch.ones(1,3,4)\n",
    "            >>> torch.matmul(a,b).shape\n",
    "            torch.Size([2,5,4])\n",
    "        \"\"\"\n",
    "    # [1,8,5,64] * [1,8,64,6] -> [1,8,5,6]\n",
    "    # scores : [batch_size, n_heads, len_q, len_k]\n",
    "    scores = torch.matmul(Q, K.transpose(2,3)) / np.sqrt(d_k) # divided by scale\n",
    "\n",
    "    \"\"\"scores.masked_fill_(attn_mask, -1e9) \n",
    "    由于scores和attn_mask维度相同,根据attn_mask中的元素值,把和attn_mask中值为True的元素的\n",
    "    位置相同的scores元素的值赋为-1e9\n",
    "    \"\"\"\n",
    "    scores.masked_fill_(attn_mask, -1e9)\n",
    "\n",
    "    # 'P'的scores元素值为-1e9, softmax值即为0\n",
    "    softmax = nn.Softmax(dim=-1) # 求行的softmax\n",
    "    attn = softmax(scores) # [1,8,6,6]\n",
    "    # [1,8,6,6] * [1,8,6,64] -> [1,8,6,64]\n",
    "    context = torch.matmul(attn, V) # [1,8,6,64]\n",
    "    return context, attn\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    # dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(d_model, d_k*n_heads) # (512, 64*8) # d_q必等于d_k\n",
    "        self.W_K = nn.Linear(d_model, d_k*n_heads) # (512, 64*8) # 保持维度不变\n",
    "        self.W_V = nn.Linear(d_model, d_v*n_heads) # (512, 64*8)\n",
    "        self.linear = nn.Linear(n_heads*d_v, d_model)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, attn_mask):\n",
    "        # dec_outputs: [batch_size, tgt_len, d_model] # [1,5,512]\n",
    "        # enc_outputs: [batch_size, src_len, d_model] # [1,6,512]\n",
    "        # dec_enc_attn_mask: [batch_size, tgt_len, src_len] # [1,5,6]\n",
    "        # q/k/v: [batch_size, len_q/k/v, d_model]\n",
    "        residual, batch_size = Q, len(Q)\n",
    "        '''用n_heads=8把512拆成64*8,在不改变计算成本的前提下,让各注意力头相互独立,更有利于学习到不同的特征'''\n",
    "        # Q_s: [batch_size, len_q, n_heads, d_q] # [1,5,8,64]\n",
    "        # new_Q_s: [batch_size, n_heads, len_q, d_q] # [1,8,5,64]\n",
    "        Q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  \n",
    "        # K_s: [batch_size, n_heads, len_k, d_k] # [1,8,6,64]\n",
    "        K_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  \n",
    "        # V_s: [batch_size, n_heads, len_k, d_v] # [1,8,6,64]\n",
    "        V_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  \n",
    "\n",
    "        # attn_mask : [1,5,6] -> [1,1,5,6] -> [1,8,5,6]\n",
    "        # attn_mask : [batch_size, n_heads, len_q, len_k]\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) \n",
    "\n",
    "        # context: [batch_size, n_heads, len_q, d_v]\n",
    "        # attn: [batch_size, n_heads, len_q(=len_k), len_k(=len_q)]\n",
    "        # context: [1,8,5,64] attn: [1,8,5,6]\n",
    "        context, attn = Scaled_Dot_Product_Attention(Q_s, K_s, V_s, attn_mask)\n",
    "        \"\"\"contiguous() 连续的\n",
    "        contiguous: view只能用在连续(contiguous)的变量上.\n",
    "        如果在view之前用了transpose, permute等,\n",
    "        需要用contiguous()来返回一个contiguous copy\n",
    "        \"\"\"\n",
    "        # context: [1,8,5,64] -> [1,5,512]\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v)\n",
    "        # context: [1,5,512] -> [1,5,512]\n",
    "        output = self.linear(context)\n",
    "        \"\"\"nn.LayerNorm(output) 样本归一化\n",
    "        和对所有样本的某一特征进行归一化的BatchNorm不同,\n",
    "        LayerNorm是对每个样本进行归一化,而不是一个特征\n",
    "\n",
    "        Tips:\n",
    "            归一化Normalization和Standardization标准化区别:\n",
    "            Normalization(X[i]) = (X[i] - np.min(X)) / (np.max(X) - np.min(X))\n",
    "            Standardization(X[i]) = (X[i] - np.mean(X)) / np.var(X)\n",
    "        \"\"\"\n",
    "        output = self.layer_norm(output + residual)\n",
    "        return output, attn \n",
    "\n",
    "class Position_wise_Feed_Forward_Networks(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        '''输出层相当于1*1卷积层,也就是全连接层'''\n",
    "        \"\"\"nn.Conv1d\n",
    "        in_channels应该理解为嵌入向量维度,out_channels才是卷积核的个数(厚度)\n",
    "        \"\"\"\n",
    "        # 512 -> 2048\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        # 2048 -> 512\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # enc_outputs: [batch_size, source_len, d_model] # [1,6,512]\n",
    "        residual = inputs \n",
    "        relu = nn.ReLU()\n",
    "        # output: 512 -> 2048 [1,2048,6]\n",
    "        output = relu(self.conv1(inputs.transpose(1, 2)))\n",
    "        # output: 2048 -> 512 [1,6,512]\n",
    "        output = self.conv2(output).transpose(1, 2)\n",
    "        return self.layer_norm(output + residual)\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = Position_wise_Feed_Forward_Networks()\n",
    "\n",
    "    def forward(self, enc_outputs, enc_attn_mask):\n",
    "        # enc_attn_mask: [1,6,6]\n",
    "        # enc_outputs to same Q,K,V\n",
    "        # enc_outputs: [batch_size, source_len, d_model] # [1, 6, 512]\n",
    "        enc_outputs, attn = self.enc_attn(enc_outputs, \\\n",
    "            enc_outputs, enc_outputs, enc_attn_mask) \n",
    "        # enc_outputs: [batch_size , len_q , d_model]\n",
    "        enc_outputs = self.pos_ffn(enc_outputs) \n",
    "        return enc_outputs, attn\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dec_attn = MultiHeadAttention()\n",
    "        self.dec_enc_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = Position_wise_Feed_Forward_Networks()\n",
    "\n",
    "    def forward(self, dec_outputs, enc_outputs, dec_attn_mask, dec_enc_attn_mask):\n",
    "        dec_outputs, dec_attn = \\\n",
    "            self.dec_attn(dec_outputs, dec_outputs, dec_outputs, dec_attn_mask)\n",
    "        # dec_outputs: [1, 5, 512]   dec_enc_attn: [1, 8, 5, 6]\n",
    "        dec_outputs, dec_enc_attn = \\\n",
    "            self.dec_enc_attn(dec_outputs, enc_outputs, enc_outputs, dec_enc_attn_mask)\n",
    "        # 相当于两个全连接层 512 -> 2048 -> 512\n",
    "        dec_outputs = self.pos_ffn(dec_outputs)\n",
    "        return dec_outputs, dec_attn, dec_enc_attn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 输入向量的嵌入矩阵\n",
    "        self.src_emb = nn.Embedding(src_dict_size, d_model).to(device)\n",
    "        # 6个编码层\n",
    "        self.layers = nn.ModuleList([EncoderLayer().to(device) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, enc_inputs): # enc_inputs: [batch_size, source_len] # [1, 6]\n",
    "        input = [src_dict[i] for i in sentences[0].split()] # [0,1,2,3,4,5]\n",
    "        \n",
    "        '''加入pos_emb的意义: 如果不加,所有位置的单词都将有完全相同的影响,体现不出序列的特点'''\n",
    "        # 可学习的输入向量嵌入矩阵 + 不可学习的序列向量矩阵\n",
    "        # enc_outputs: [1, 6, 512]\n",
    "        '''embbeding和linear不同,emb之后会加一个维度'''\n",
    "        enc_outputs = self.src_emb(enc_inputs) + position_encoding[input] \n",
    "        # 屏蔽P,返回一个 [batch_size,src_len,src_len]=[1,6,6]的二值矩阵\n",
    "        enc_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs, src_dict)\n",
    "\n",
    "        enc_attns = []\n",
    "        for layer in self.layers:\n",
    "            # enc_outputs既是输入,也是输出\n",
    "            enc_outputs, enc_attn = layer(enc_outputs, enc_attn_mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        return enc_outputs, enc_attns\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tgt_emb = nn.Embedding(tgt_dict_size, d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer() for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, dec_inputs, enc_inputs, enc_outputs): \n",
    "        # dec_inputs : [batch_size, target_len] [1,5]\n",
    "        input = [tgt_dict[i] for i in sentences[1].split()]\n",
    "        dec_outputs = self.tgt_emb(dec_inputs)\n",
    "\n",
    "        # 为输入句子的每一个单词(不去重)加入序列信息\n",
    "        # dec_outputs: [1, 5, 512]\n",
    "        dec_outputs = dec_outputs + position_encoding[input] \n",
    "        \n",
    "        # 屏蔽pad字符,返回一个 [batch_size,tgt_len,tgt_len]=[1,5,5]的二值矩阵\n",
    "        dec_attn_mask = get_attn_pad_mask(dec_inputs, dec_inputs, tgt_dict)\n",
    "        \n",
    "        # 屏蔽pad字符和之后时刻的信息\n",
    "        for i in range(0, len(dec_inputs[0])):\n",
    "            for j in range(i+1, len(dec_inputs[0])):\n",
    "                dec_attn_mask[0][i][j] = True # 使softmax值为0\n",
    "        \n",
    "        # 第二个多注意力机制,输入来自encoder和decoder 屏蔽encoder中的pad字符\n",
    "        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs, src_dict) # [1,5,6]\n",
    "\n",
    "        dec_attns, dec_enc_attns = [], []\n",
    "        for layer in self.layers:\n",
    "            dec_outputs, dec_attn, dec_enc_attn = \\\n",
    "                layer(dec_outputs, enc_outputs, dec_attn_mask, dec_enc_attn_mask)\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return dec_outputs, dec_attns, dec_enc_attns\n",
    "\n",
    "'''2.构建Transformer模型'''\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.projection = nn.Linear(d_model, tgt_dict_size, bias=False)\n",
    "    def forward(self, dec_inputs, enc_inputs):\n",
    "        enc_outputs, enc_attns = self.encoder(enc_inputs)\n",
    "        dec_outputs, dec_attns, dec_enc_attns \\\n",
    "            = self.decoder(dec_inputs, enc_inputs, enc_outputs)\n",
    "        # model_outputs: [batch_size, tgt_len, tgt_dict_size]\n",
    "        model_outputs = self.projection(dec_outputs) # [1,5,7]\n",
    "        model_outputs = model_outputs.squeeze(0)\n",
    "\n",
    "        return model_outputs, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "'''贪婪搜索'''\n",
    "def Greedy_Search(model, enc_inputs, start_symbol): # start_symbol = tgt_dict['S']\n",
    "    '''\n",
    "    依次生成tgt_len个目标单词,每生成一个单词都要进行一次完整的transformer计算,\n",
    "    最后依次取第i个位置上概率最大的单词,生成新的dec_inputs\n",
    "    贪婪搜索输出之后,新的dec_inputs重新和其他参数输入模型,最后生成predict\n",
    "    '''\n",
    "    padding_inputs = 'S' + ' P' * (tgt_len-1)\n",
    "    # [1,5]\n",
    "    dec_inputs = torch.Tensor([[tgt_dict[i] for i in padding_inputs.split()]]).to(device).type_as(enc_inputs.data)\n",
    "    next_symbol = start_symbol\n",
    "    i = 0\n",
    "\n",
    "    while True:\n",
    "        '''\n",
    "        由enc_inputs和'S P P P P'生成i,然后i赋值给下一轮的dec_inputs:'S i P P P'\n",
    "        然后生成want->'S i want P P' ······,生成beer->'S i want a beer'\n",
    "        其实就是生成target_batch,然后错位赋值给dec_inputs,这个刚好符合训练模型时\n",
    "        target_batch对应sentence[2],dec_inputs对应的sentence[1],enc_inputs对应的sentence[0]\n",
    "        '''\n",
    "        dec_inputs[0][i] = next_symbol\n",
    "        outputs, _, _, _ = model(dec_inputs, enc_inputs)    \n",
    "        predict = outputs.squeeze(0).max(dim=-1, keepdim=False)[1]\n",
    "        next_symbol = predict[i].item()\n",
    "        i = i + 1\n",
    "        # 超过最大长度或者有终止符则退出循环\n",
    "        if next_symbol == tgt_dict['E'] or i > max_len:\n",
    "            break\n",
    "    outputs, _, _, _ = model(dec_inputs, enc_inputs)    \n",
    "    predict = outputs.squeeze(0).max(dim=-1, keepdim=False)[1]\n",
    "    return predict\n",
    "\n",
    "# 为束搜索构造连续重复矩阵\n",
    "def repeat(inputs,k=2):\n",
    "    \"\"\"连续重复\n",
    "    Examples:\n",
    "        >>> a = torch.tensor([[1,2,3],[4,5,6]])\n",
    "        >>> print(repeat(a))\n",
    "        [[1,2,3],\n",
    "         [1,2,3],\n",
    "         [4,5,6],\n",
    "         [4,5,6]]\n",
    "        >>> import torch\n",
    "        >>> print(torch.repeat(a))\n",
    "        [[1,2,3],\n",
    "         [4,5,6],\n",
    "         [1,2,3],\n",
    "         [4,5,6]]        \n",
    "    \"\"\"\n",
    "    tmp = torch.zeros(inputs.size(0)*k, inputs.size(1), inputs.size(2)).type_as(inputs.data)\n",
    "    pos = 0\n",
    "    while pos < len(inputs):\n",
    "        tmp[pos*k] = inputs[pos]\n",
    "        for i in range(1,k):\n",
    "            tmp[pos*k+i] = inputs[pos]\n",
    "        pos = pos + 1\n",
    "    return tmp\n",
    "\n",
    "'''束搜索'''\n",
    "def Beam_Search(model, enc_inputs, start_symbol, k=2):\n",
    "    padding_inputs = 'S' + (tgt_len-1)*' P'\n",
    "    # dec_inputs:[1,5]\n",
    "    dec_inputs = torch.Tensor([[tgt_dict[i] for i in padding_inputs.split()]]).to(device).type_as(enc_inputs.data)\n",
    "    # all_dec_inputs用来存储每一种dec_inputs\n",
    "    # all_dec_inputs:[1,1,5]\n",
    "    all_dec_inputs = dec_inputs.unsqueeze(0)\n",
    "    # 由于第一个单词被定为'S',所以其实每轮生成的实际结果数是2,4,8,16,16(最后一轮复制为32,但因为马上就结束循环,不会再分别赋值)\n",
    "    for i in range(tgt_len):\n",
    "        # 每一个单词,all_dec_inputs都要重复k次,存放新生成的k**i个结果\n",
    "        # 注意,repeat函数实现的是连续重复,而不是toch.repeat那样的整体间断重复\n",
    "        all_dec_inputs = repeat(all_dec_inputs,k) \n",
    "        \n",
    "        # 对于第i个单词,以步长k遍历all_dec_inputs(即前一轮的所有dec_inputs)\n",
    "        # 分别将其作为模型输入\n",
    "        for j in range(0,len(all_dec_inputs),k):\n",
    "            # print(j)\n",
    "            dec_inputs = all_dec_inputs[j] \n",
    "            outputs, _, _, _ = model(dec_inputs, enc_inputs)\n",
    "            # 排序,得到索引\n",
    "            indices = outputs.sort(descending=True).indices # [5,7]\n",
    "            # 提取第i个单词的第k个可能,赋值给all_dec_inputs的第j+pos个样本\n",
    "            # (因为连续重复,所以all_dec_inputs中第j个dec_inputs生成的\n",
    "            # 输出就分布在j~j+k个dec_inputs)\n",
    "            for pos in range(k):\n",
    "                if i < tgt_len - 1:\n",
    "                    # i+1表示留给下一轮用,这和贪婪搜索中的思想一致\n",
    "                    all_dec_inputs[j+pos][0][i+1] = indices[i][pos]\n",
    "    print(all_dec_inputs)\n",
    "    '''评判所有dec_inputs,选出输出概率最大的'''\n",
    "    result = []\n",
    "    for dec_inputs in all_dec_inputs:\n",
    "        sum = 0\n",
    "        outputs, _, _, _ = model(dec_inputs, enc_inputs) \n",
    "        indexs = outputs.squeeze(0).max(dim=-1, keepdim=False)[1]\n",
    "        for i in range(tgt_len):\n",
    "            # 计算各个样本输出的单词概率之和\n",
    "            sum = sum + outputs.data[i][indexs[i]]\n",
    "        result.append(sum.item())\n",
    "    '''可能过拟合输出错误解'''\n",
    "    max_index = result.index(max(result))\n",
    "    predict = all_dec_inputs[max_index]\n",
    "    return predict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    chars = 30 * '*'\n",
    "    # sentences = ['ich mochte ein bier P', 'S i want a beer', 'i want a beer E']\n",
    "    sentences = ['我 要 喝 啤 酒 P', 'S i want a beer', 'i want a beer E']\n",
    "    device = ['cuda:0' if torch.cuda.is_available() else 'cpu'][0]\n",
    "    d_model = 512  # Embedding Size 嵌入向量维度\n",
    "    d_ff = 2048  # FeedForward dimension \n",
    "    d_k = d_v = 64  # dimension of K(=Q), V \n",
    "    n_layers = 6  # number of Encoder of Decoder Layer 编解码器层数\n",
    "    n_heads = 8  # number of heads in Multi-Head Attention 多注意力机制头数\n",
    "    max_len = 5\n",
    "\n",
    "    '''1.数据预处理'''\n",
    "    src_dict,src_dict_size,tgt_dict,number_dict,tgt_dict_size,src_len,tgt_len = pre_process(sentences)\n",
    "    position_encoding = get_position_encoding_table(max(src_dict_size, tgt_dict_size), d_model)\n",
    "    enc_inputs, dec_inputs, target_batch = make_batch(sentences)\n",
    "\n",
    "    '''2.构建模型'''\n",
    "    model = Transformer()\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Adam的效果非常不好\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "    if os.path.exists('model_param.pt') == True:\n",
    "        # 加载模型参数到模型结构\n",
    "        model.load_state_dict(torch.load('model_param.pt', map_location=device))\n",
    "\n",
    "    '''3.训练'''\n",
    "    print('{}\\nTrain\\n{}'.format('*'*30, '*'*30))\n",
    "    loss_record = []\n",
    "    for epoch in range(1000):\n",
    "        optimizer.zero_grad()\n",
    "        outputs, enc_attns, dec_attns, dec_enc_attns \\\n",
    "            = model(dec_inputs, enc_inputs)\n",
    "        outputs = outputs.to(device)\n",
    "        loss = criterion(outputs, target_batch.contiguous().view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if loss >= 0.01: # 连续30轮loss小于0.01则提前结束训练\n",
    "            loss_record = []\n",
    "        else:\n",
    "            loss_record.append(loss.item())\n",
    "            if len(loss_record) == 30:\n",
    "                torch.save(model.state_dict(), 'model_param.pt')\n",
    "                break    \n",
    "\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'Loss = {:.6f}'.format(loss))\n",
    "            torch.save(model.state_dict(), 'model_param.pt')\n",
    "\n",
    "    '''4.测试'''\n",
    "    print('{}\\nTest\\n{}'.format('*'*30, '*'*30))\n",
    "    # 贪婪算法\n",
    "    predict = Greedy_Search(model, enc_inputs, start_symbol=tgt_dict[\"S\"])\n",
    "    # 束搜索算法\n",
    "    predict = Beam_Search(model, enc_inputs, start_symbol=tgt_dict[\"S\"])\n",
    "    \n",
    "    for word in sentences[0].split():\n",
    "        if word not in ['S', 'P', 'E']:\n",
    "            print(word, end='')\n",
    "    print(' ->',end=' ')\n",
    "    \n",
    "    for i in predict.data.squeeze():\n",
    "        if number_dict[int(i)] not in ['S', 'P', 'E'] :\n",
    "            print(number_dict[int(i)], end=' ')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8384712f827c73b71ec1d871aaffaaf3604c18a78a335b5adcc2028fc4cb4b02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
