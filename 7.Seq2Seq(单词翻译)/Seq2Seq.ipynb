{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task: 基于Seq2Seq的单词翻译\n",
    "Author: ChengJunkai @github.com/Cheng0829\n",
    "Email: chengjunkai829@gmail.com\n",
    "Date: 2022/09/11\n",
    "Reference: Tae Hwan Jung(Jeff Jung) @graykode\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch, time, os, sys\n",
    "import torch.nn as nn\n",
    "\n",
    "# S: 表示开始进行解码输入的符号。\n",
    "# E: 表示结束进行解码输出的符号。\n",
    "# P: 当前批次数据大小小于时间步长时将填充空白序列的符号\n",
    "\n",
    "'''1.数据预处理'''\n",
    "def pre_process(seq_data):\n",
    "    chars = 'SEPabcdefghijklmnopqrstuvwxyz'\n",
    "    char_arr = [char for char in chars]\n",
    "    \n",
    "    word_dict = {i:n for i,n in enumerate(char_arr)} \n",
    "    # 字符字典\n",
    "    num_dict = {n:i for i,n in enumerate(char_arr)}    \n",
    "    # 字符种类\n",
    "    n_class = len(num_dict)\n",
    "    # 样本数\n",
    "    batch_size = len(seq_data)\n",
    "    return char_arr, word_dict, num_dict, n_class, batch_size\n",
    "\n",
    "'''根据句子数据,构建词元的嵌入向量及目标词索引'''\n",
    "def make_batch(seq_data):\n",
    "    input_batch, output_batch, target_batch = [], [], []\n",
    "\n",
    "    for seq in seq_data:\n",
    "        for i in range(2):\n",
    "            # 把每个单词补充到时间步长度\n",
    "            seq[i] = seq[i] + 'P' * (n_step - len(seq[i]))\n",
    "\n",
    "        input = [num_dict[n] for n in seq[0]]\n",
    "        # output是decoder的输入,所以加上开始解码输入的符号\n",
    "        output = [num_dict[n] for n in ('S' + seq[1])]\n",
    "        # target是decoder的输出,所以加上开始解码输出的符号\n",
    "        target = [num_dict[n] for n in (seq[1] + 'E')]\n",
    "\n",
    "        input_batch.append(np.eye(n_class)[input])\n",
    "        output_batch.append(np.eye(n_class)[output])\n",
    "        target_batch.append(target) # not one-hot\n",
    "\n",
    "    '''input_batch用于编码器输入, output_batch用于解码器输入, target_batch用于比较计算误差'''\n",
    "    # [样本数,时间步长度,嵌入向量维度] -> [6,5,29] \n",
    "    input_batch = torch.FloatTensor(np.array(input_batch)).to(device) \n",
    "    # [样本数,时间步长度+1,嵌入向量维度] -> [6,6,29] \n",
    "    output_batch = torch.FloatTensor(np.array(output_batch)).to(device) \n",
    "    # [样本数,时间步长度+1] -> [6,6]\n",
    "    target_batch = torch.LongTensor (np.array(target_batch)).to(device)  \n",
    " \n",
    "    return input_batch, output_batch, target_batch\n",
    "\n",
    "'''2.构建模型'''\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.decoder = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5)\n",
    "        self.fc = nn.Linear(n_hidden, n_class)\n",
    "\n",
    "    '''编码器5个时间步,解码器六个:一个时间步对应一个单词字母'''\n",
    "    def forward(self, encoder_input, encoder_hidden, decoder_input):\n",
    "        '''\n",
    "            encoder_input: input_batch\n",
    "            encoder_hidden: hidden\n",
    "            decoder_input: output_batch        \n",
    "        '''\n",
    "        # encoder_input: [n_step, batch_size, n_class] -> [5,6,29]\n",
    "        encoder_input = encoder_input.transpose(0, 1)\n",
    "        # decoder_input: [n_step, batch_size, n_class] -> [6,6,29]\n",
    "        decoder_input = decoder_input.transpose(0, 1)\n",
    "\n",
    "        '''编码器输出作为解码器输入的hidden'''\n",
    "        # hidden最后只从一个单元里输出,所以第一维是1\n",
    "        # encoder_states : [num_layers(=1)*num_directions(=1), batch_size, n_hidden] # [1,6,128]\n",
    "        _, encoder_states = self.encoder(encoder_input, encoder_hidden)\n",
    "        encoder_states = encoder_states.to(device)\n",
    "        '''解码器输出'''\n",
    "        # outputs : [n_step+1(=6), batch_size, num_directions(=1)*n_hidden(=128)] # [6,6,128]\n",
    "        outputs, _ = self.decoder(decoder_input, encoder_states)\n",
    "        outputs = outputs.to(device)\n",
    "        '''全连接层'''\n",
    "        # output : [n_step+1(=6), batch_size, n_class]\n",
    "        output = self.fc(outputs) # [6,6,29]\n",
    "        return output\n",
    "\n",
    "def translate(input_word):\n",
    "    input_batch, output_batch = [], []\n",
    "    # 把每个单词补充到时间步长度\n",
    "    input_word = input_word + 'P' * (n_step - len(input_word))\n",
    "    # 换成序号\n",
    "    input = [num_dict[n] for n in input_word] # \n",
    "    # 除了一个表示开始解码输入的符号,其余均为空白符号\n",
    "    output = [num_dict[n] for n in 'S'+'P'*n_step]\n",
    "\n",
    "    input_batch = np.eye(n_class)[input]\n",
    "    output_batch = np.eye(n_class)[output]\n",
    "\n",
    "    input_batch = torch.FloatTensor(np.array(input_batch)).unsqueeze(0).to(device)\n",
    "    output_batch = torch.FloatTensor(np.array(output_batch)).unsqueeze(0).to(device)\n",
    "    '''样本集为1'''\n",
    "    # hidden : [num_layers*num_directions, batch_size, n_hidden] [1,1,128]\n",
    "    hidden = torch.zeros(1, 1, n_hidden).to(device)\n",
    "    '''output : [n_step+1(=6), batch_size, n_class] [6,1,29]'''\n",
    "    output = model(input_batch, hidden, output_batch) # [6,1,29]\n",
    "    \n",
    "    '''torch.tensor.data.max(dim,keepdim) 用于找概率最大的输出值及其索引\n",
    "    Args:\n",
    "        dim (int): 在哪一个维度求最大值\n",
    "        keepdim (Boolean): 保持维度. \n",
    "            keepdim=True:当tensor维度>1时,得到的索引和输出值仍然保持原来的维度\n",
    "            keepdim=False:当tensor维度>1时,得到的索引和输出值为1维\n",
    "    '''\n",
    "    '''dim=2:在第2维求最大值  [1]:只需要索引'''\n",
    "    predict = output.data.max(2, keepdim=True)[1] # select n_class dimension\n",
    "    '''由于predict中元素全为索引整数,所以即使有几个中括号,仍可以直接作为char_arr的索引'''\n",
    "    decoded = [char_arr[i] for i in predict] # ['m', 'e', 'n', 'P', 'P', 'E']\n",
    "\n",
    "    '''清除特殊字符'''\n",
    "    '''训练集的target均以E结尾,所以模型输出最后一个值也会是E'''\n",
    "    if 'E' in decoded:\n",
    "        end = decoded.index('E') # 5\n",
    "        decoded = decoded[:end] # 删除结束符及之后的所有字符\n",
    "    else:\n",
    "        return # 报错\n",
    "    while(True):\n",
    "        if 'P' in decoded:\n",
    "            del decoded[decoded.index('P')] # 删除空白符\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # 把列表元素合成字符串\n",
    "    translated = ''.join(decoded) \n",
    "    return translated\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    chars_print = '*' * 30\n",
    "    n_step = 5 # (样本单词均不大于5,所以n_step=5)\n",
    "    n_hidden = 128\n",
    "    device = ['cuda:0' if torch.cuda.is_available() else 'cpu'][0]\n",
    "    # 单词序列\n",
    "    seq_data = [['man', 'men'], ['black', 'white'], ['king', 'queen'], \\\n",
    "                ['girl', 'boy'], ['up', 'down'], ['high', 'low']]\n",
    "\n",
    "    '''1.数据预处理'''\n",
    "    char_arr, word_dict, num_dict, n_class, batch_size = pre_process(seq_data)\n",
    "    input_batch, output_batch, target_batch = make_batch(seq_data)\n",
    "\n",
    "    '''2.构建模型'''\n",
    "    model = Seq2Seq()\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    if os.path.exists('model_param.pt') == True:\n",
    "        # 加载模型参数到模型结构\n",
    "        model.load_state_dict(torch.load('model_param.pt', map_location=device))\n",
    "\n",
    "    '''3.训练'''\n",
    "    print('{}\\nTrain\\n{}'.format('*'*30, '*'*30))\n",
    "    loss_record = []\n",
    "    for epoch in range(10000):\n",
    "        # make hidden shape [num_layers * num_directions, batch_size, n_hidden] [1,6,128]\n",
    "        hidden = torch.zeros(1, batch_size, n_hidden).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # input_batch : [样本数, 时间步长度, 嵌入向量维度]\n",
    "        # output_batch : [样本数, 时间步长度+1, 嵌入向量维度]\n",
    "        # target_batch : [样本数, 时间步长度+1] \n",
    "        output = model(input_batch, hidden, output_batch) # [6,6,29]\n",
    "        # output : [max_len+1, batch_size, n_class]\n",
    "        output = output.transpose(0, 1) # [batch_size, max_len+1(=6), n_class] [6,6,29]\n",
    "        \n",
    "        '''\n",
    "        criterion的输入应该是output二维,target_batch一维,此实验不是这样,\n",
    "        一个单词样本分为几个字母,每个字母指定一个字母输出,因此target_batch是二维\n",
    "        所以要遍历相加.\n",
    "        '''\n",
    "        loss = 0\n",
    "        for i in range(0, len(target_batch)):\n",
    "            '''output: [6,6,29] target_batch:[6,6]'''\n",
    "            loss = loss + criterion(output[i], target_batch[i])\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "        if loss >= 0.0001: # 连续30轮loss小于0.01则提前结束训练\n",
    "            loss_record = []\n",
    "        else:\n",
    "            loss_record.append(loss.item())\n",
    "            if len(loss_record) == 30:\n",
    "                torch.save(model.state_dict(), 'model_param.pt')\n",
    "                break    \n",
    "\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'Loss = {:.10f}'.format(loss))\n",
    "            torch.save(model.state_dict(), 'model_param.pt')\n",
    "\n",
    "    '''4.预测'''\n",
    "    print('{}\\nTest\\n{}'.format('*'*30, '*'*30))\n",
    "    test_words = ['man','men','king','black','upp']\n",
    "    for word in test_words:\n",
    "        print('%s ->'%word, translate(word))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8384712f827c73b71ec1d871aaffaaf3604c18a78a335b5adcc2028fc4cb4b02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
